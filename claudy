#!/bin/bash
# claudy - Containerized Claude CLI launcher
# Opens Claude inside a tmux session within a hal-9000 container
# See: .pm/BEADS.md (CLAUDY-IMPL-1-1) for implementation details

set -Eeuo pipefail

readonly SCRIPT_NAME="claudy"
readonly SCRIPT_VERSION="0.6.0"
readonly HAL9000_HOME="${HOME}/.hal9000"
readonly PARENT_CONTAINER="hal9000-parent"
readonly PARENT_IMAGE="ghcr.io/hellblazer/hal-9000:parent"
# CLAUDE_HOME can be overridden by environment variable or --claude-home argument
# Default: ~/.claude (same as Claude Code)
readonly DEFAULT_CLAUDE_HOME="${HOME}/.claude"
readonly DEFAULT_PROFILE="base"
readonly CONTAINER_IMAGE_BASE="ghcr.io/hellblazer/hal-9000"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

#==============================================================================
# Error handling
#==============================================================================

error() {
    echo -e "${RED}✗ Error: $*${NC}" >&2
    exit 1
}

warn() {
    echo -e "${YELLOW}⚠ Warning: $*${NC}" >&2
}

info() {
    echo -e "${BLUE}ℹ $*${NC}"
}

success() {
    echo -e "${GREEN}✓ $*${NC}"
}

#==============================================================================
# Help and diagnostics
#==============================================================================

show_help() {
    cat << 'EOF'
claudy - Containerized Claude CLI

USAGE:
  claudy [OPTIONS] [DIRECTORY]
  claudy daemon <command>
  claudy pool <command>

DAEMON COMMANDS:
  claudy daemon start    Start the orchestrator (parent container with ChromaDB)
  claudy daemon stop     Stop the orchestrator
  claudy daemon status   Show orchestrator status and worker count
  claudy daemon restart  Restart the orchestrator

POOL COMMANDS:
  claudy pool start      Start the worker pool manager
  claudy pool stop       Stop the worker pool manager
  claudy pool status     Show pool status (warm/busy workers)
  claudy pool scale <n>  Scale to n warm workers
  claudy pool cleanup    Force cleanup of idle workers

OPTIONS:
  --setup                Interactive setup for API key (first-time setup)
  --profile PROFILE      Profile: base, python, node, java (default: auto-detect)
  --shell, -s            Start bash shell instead of Claude
  --name NAME            Custom session name (default: auto from directory)
  --claude-home PATH     Path to Claude config directory (default: $CLAUDE_HOME or ~/.claude)
  --api-key KEY          Anthropic API key (or set ANTHROPIC_API_KEY env var)
  --detach, -d           Don't attach to tmux session after launch
  --via-parent           Launch worker via parent container (requires daemon running)
  --legacy               Force single-container mode (v0.5.x behavior, deprecated)
  --verify               Verify prerequisites and exit
  --diagnose             Show diagnostic information
  --help, -h             Show this help message
  --version, -v          Show version

ARGUMENTS:
  DIRECTORY              Project directory (default: current directory)

AUTHENTICATION (required):
  Containerized Claude requires ANTHROPIC_API_KEY environment variable.
  Host keychain credentials cannot be transferred to containers.

  Option 1 - API Key (recommended):
    export ANTHROPIC_API_KEY=sk-ant-api03-...
    Get key at: https://console.anthropic.com/settings/keys

  Option 2 - Long-lived token:
    claude setup-token   # Run once on host
    # Copy the token and set as ANTHROPIC_API_KEY

  Add to ~/.bashrc or ~/.zshrc to persist.

EXAMPLES:
  claudy                                    # Current dir, auto-detect profile
  claudy --profile python                   # Force Python profile
  claudy ~/projects/myapp                   # Specific directory
  claudy --shell -d                         # Detached shell session
  claudy --api-key sk-ant-xxx123            # Use specific API key
  claudy --diagnose                         # Check setup

DAEMON EXAMPLES:
  claudy daemon start                       # Start the orchestrator
  claudy daemon status                      # Check status and workers
  claudy daemon stop                        # Stop the orchestrator
  claudy --via-parent ~/project             # Launch worker via parent (DinD mode)

ENVIRONMENT:
  ANTHROPIC_API_KEY      API key for Claude authentication
  CLAUDE_HOME            Override Claude config directory (default: ~/.claude)
  HAL9000_HOME           Override session storage (default: ~/.hal9000)
  DOCKER_SOCKET          Docker socket path (default: /var/run/docker.sock)

CONTAINER CONFIG:
  Config is mounted at /claude with CLAUDE_CONFIG_DIR=/claude
  Host UID/GID passed via USER_UID/USER_GID for file permissions

For more information, see documentation at:
  https://github.com/hellblazer/hal-9000/blob/main/README.md
EOF
}

show_version() {
    echo "claudy version $SCRIPT_VERSION"
    echo "hal-9000 plugin version 1.4.0+"
}

setup_auth() {
    echo ""
    info "Claudy Authentication Setup"
    echo ""

    # Check if already configured
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        success "ANTHROPIC_API_KEY is already set"
        echo "  Current value: ${ANTHROPIC_API_KEY:0:15}..."
        echo ""
        read -p "Replace with a new key? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Keeping existing configuration"
            exit 0
        fi
    fi

    echo "Enter your Anthropic API key (or 'q' to quit):"
    echo "  Get one at: https://console.anthropic.com/settings/keys"
    echo ""
    read -p "API Key: " api_key_input

    if [[ "$api_key_input" == "q" ]] || [[ -z "$api_key_input" ]]; then
        info "Setup cancelled"
        exit 0
    fi

    # Validate format
    if [[ ! "$api_key_input" =~ ^sk-ant- ]]; then
        warn "Key doesn't look like an Anthropic API key (expected sk-ant-...)"
        read -p "Continue anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi

    # Detect shell config file
    local shell_rc=""
    if [[ -n "${ZSH_VERSION:-}" ]] || [[ "$SHELL" == *"zsh"* ]]; then
        shell_rc="$HOME/.zshrc"
    else
        shell_rc="$HOME/.bashrc"
    fi

    echo ""
    info "Where to save the API key?"
    echo "  1) $shell_rc (recommended - persists across sessions)"
    echo "  2) Current session only (export now, won't persist)"
    echo "  3) Cancel"
    echo ""
    read -p "Choice [1]: " -n 1 -r choice
    echo ""

    case "${choice:-1}" in
        1)
            # Check if already in file
            if grep -q "ANTHROPIC_API_KEY" "$shell_rc" 2>/dev/null; then
                warn "ANTHROPIC_API_KEY already exists in $shell_rc"
                read -p "Replace it? [y/N] " -n 1 -r
                echo ""
                if [[ $REPLY =~ ^[Yy]$ ]]; then
                    # Remove old line and add new
                    sed -i.bak '/export ANTHROPIC_API_KEY/d' "$shell_rc"
                else
                    info "Setup cancelled"
                    exit 0
                fi
            fi

            echo "" >> "$shell_rc"
            echo "# Anthropic API key for Claude Code (added by claudy)" >> "$shell_rc"
            echo "export ANTHROPIC_API_KEY=\"$api_key_input\"" >> "$shell_rc"

            success "Added to $shell_rc"
            echo ""
            info "To use now, run:"
            echo "  source $shell_rc"
            echo ""
            info "Or start a new terminal session."
            ;;
        2)
            export ANTHROPIC_API_KEY="$api_key_input"
            success "API key set for current session"
            echo ""
            info "Note: This won't persist. Run 'claudy --setup' again to save permanently."
            ;;
        *)
            info "Setup cancelled"
            exit 0
            ;;
    esac

    echo ""
    success "Setup complete! You can now run: claudy"
}

show_diagnostics() {
    info "Claudy Diagnostics"
    echo ""

    # Check Docker
    if command -v docker &> /dev/null; then
        DOCKER_VERSION=$(docker --version)
        success "Docker: $DOCKER_VERSION"
    else
        error "Docker not found. Install Docker to use claudy."
    fi

    # Check tmux
    if command -v tmux &> /dev/null; then
        TMUX_VERSION=$(tmux -V)
        success "tmux: $TMUX_VERSION"
    else
        warn "tmux not found. Install tmux for full functionality."
    fi

    # Check bash version
    BASH_VERSION="${BASH_VERSION%.*}"
    if [[ "${BASH_VERSION}" =~ ^[5-9]|[0-9]{2,} ]]; then
        success "Bash: $BASH_VERSION (OK)"
    else
        warn "Bash version: $BASH_VERSION (recommend 5.0+)"
    fi

    # Check authentication
    echo ""
    info "Authentication:"
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        local key_preview="${ANTHROPIC_API_KEY:0:10}..."
        success "  ANTHROPIC_API_KEY: Set ($key_preview)"
    else
        warn "  ANTHROPIC_API_KEY: Not set"
    fi

    # Check Claude home
    if [[ -d "$CLAUDE_HOME" ]]; then
        success "Claude home: $CLAUDE_HOME (exists)"

        # Check for config files
        local config_count=0
        [[ -f "$CLAUDE_HOME/settings.json" ]] && ((config_count++))
        [[ -d "$CLAUDE_HOME/agents" ]] && ((config_count++))
        [[ -d "$CLAUDE_HOME/mcp-servers" ]] && ((config_count++))
        [[ -f "$CLAUDE_HOME/CLAUDE.md" ]] && ((config_count++))
        info "  → Config files found: $config_count"

        # Check for agents
        if [[ -d "$CLAUDE_HOME/agents" ]]; then
            local agent_count
            agent_count=$(find "$CLAUDE_HOME/agents" -maxdepth 1 -name "*.md" 2>/dev/null | wc -l | tr -d ' ')
            info "  → Agents: $agent_count"
        fi

        # Check for MCP servers
        if [[ -d "$CLAUDE_HOME/mcp-servers/servers" ]]; then
            local mcp_count
            mcp_count=$(find "$CLAUDE_HOME/mcp-servers/servers" -name "*.json" 2>/dev/null | wc -l | tr -d ' ')
            info "  → MCP servers: $mcp_count"
        fi
    else
        warn "Claude home: $CLAUDE_HOME (not configured)"
    fi

    # Check hal9000 storage
    echo ""
    if [[ -d "$HAL9000_HOME" ]]; then
        SESSION_COUNT=$(find "$HAL9000_HOME/claude" -maxdepth 1 -type d 2>/dev/null | wc -l)
        success "hal9000 home: $HAL9000_HOME"
        info "  → Active sessions: $((SESSION_COUNT - 1))"
    else
        info "hal9000 home: $HAL9000_HOME (not yet created)"
    fi

    # Check Docker socket
    if [[ -S /var/run/docker.sock ]]; then
        success "Docker socket: /var/run/docker.sock"
    elif [[ -S "$HOME/.docker/run/docker.sock" ]]; then
        success "Docker socket: $HOME/.docker/run/docker.sock"
    else
        warn "Docker socket not found"
    fi

    # Check for container image
    echo ""
    if docker inspect "${CONTAINER_IMAGE_BASE}:base" &> /dev/null; then
        success "Container image: ${CONTAINER_IMAGE_BASE}:base (available)"
    else
        warn "Container image: ${CONTAINER_IMAGE_BASE}:base (not found - run: make build-base)"
    fi

    echo ""
    info "System: $(uname -s) $(uname -m)"
    info "Current directory: $(pwd)"

    # Auth guidance
    echo ""
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        info "Authentication options:"
        echo "  1. Set ANTHROPIC_API_KEY environment variable"
        echo "  2. Run 'claudy --shell' then 'claude /login' inside container"
        echo "  3. Run 'claude setup-token' on host for long-lived token"
    fi

    # Container config info
    echo ""
    info "Container config:"
    echo "  CLAUDE_CONFIG_DIR=/claude (mounted from session)"
    echo "  USER_UID/USER_GID passed for file permissions"
}

#==============================================================================
# Daemon commands
#==============================================================================

daemon_status() {
    info "HAL-9000 Orchestrator Status"
    echo ""

    # Check if parent container exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        local status
        status=$(docker inspect --format '{{.State.Status}}' "$PARENT_CONTAINER" 2>/dev/null || echo "unknown")

        case "$status" in
            running)
                success "Parent container: Running"

                # Get uptime
                local started_at
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$PARENT_CONTAINER" 2>/dev/null || echo "")
                if [[ -n "$started_at" ]]; then
                    info "  Started: $started_at"
                fi

                # Check ChromaDB server
                if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                    success "  ChromaDB server: Running on port 8000"
                else
                    warn "  ChromaDB server: Not responding"
                fi

                # Count workers
                local worker_count
                worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')
                info "  Active workers: $worker_count"

                # List workers
                if [[ "$worker_count" -gt 0 ]]; then
                    echo ""
                    info "Workers:"
                    docker ps --filter "name=hal9000-worker" --format '  {{.Names}}: {{.Status}}' 2>/dev/null
                fi
                ;;
            exited)
                warn "Parent container: Stopped"
                local exit_code
                exit_code=$(docker inspect --format '{{.State.ExitCode}}' "$PARENT_CONTAINER" 2>/dev/null || echo "?")
                info "  Exit code: $exit_code"
                ;;
            *)
                warn "Parent container: $status"
                ;;
        esac
    else
        warn "Parent container: Not found"
        info "  Run 'claudy daemon start' to create"
    fi

    # Check for shared volumes
    echo ""
    info "Shared volumes:"
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if docker volume inspect "$vol" >/dev/null 2>&1; then
            success "  $vol: exists"
        else
            info "  $vol: not created"
        fi
    done
}

daemon_start() {
    info "Starting HAL-9000 orchestrator..."

    # Check if already running
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        success "Orchestrator already running"
        daemon_status
        return 0
    fi

    # Remove stopped container if exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped parent container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1 || true
    fi

    # Pull image if not available
    if ! docker image inspect "$PARENT_IMAGE" >/dev/null 2>&1; then
        info "Pulling parent image: $PARENT_IMAGE"
        docker pull "$PARENT_IMAGE" || {
            warn "Could not pull image, trying local build..."
            local docker_dir="${HAL9000_HOME}/../hal-9000/plugins/hal-9000/docker"
            if [[ -f "$docker_dir/Dockerfile.parent" ]]; then
                docker build -f "$docker_dir/Dockerfile.parent" -t "$PARENT_IMAGE" "$docker_dir"
            else
                error "Parent image not available. Build with: make build-parent"
            fi
        }
    fi

    # Create shared volumes if they don't exist
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if ! docker volume inspect "$vol" >/dev/null 2>&1; then
            info "Creating volume: $vol"
            docker volume create "$vol" >/dev/null
        fi
    done

    # Ensure hal9000 directories exist
    mkdir -p "$HAL9000_HOME/sessions"
    mkdir -p "$HAL9000_HOME/logs"
    mkdir -p "$HAL9000_HOME/config"

    # Start parent container
    info "Starting parent container..."
    local docker_args=(
        docker run -d
        --name "$PARENT_CONTAINER"
        --restart unless-stopped
        -v /var/run/docker.sock:/var/run/docker.sock
        -v "${HAL9000_HOME}:/root/.hal9000"
        -v hal9000-chromadb:/data/chromadb
        -v hal9000-memorybank:/data/membank
        -v hal9000-plugins:/data/plugins
    )

    # Pass through API key if set
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e ANTHROPIC_API_KEY)
    fi

    docker_args+=("$PARENT_IMAGE")

    if "${docker_args[@]}" >/dev/null; then
        # Wait for ChromaDB to start
        info "Waiting for ChromaDB server..."
        local max_wait=30
        local waited=0
        while [[ $waited -lt $max_wait ]]; do
            if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                success "ChromaDB server ready"
                break
            fi
            sleep 1
            ((waited++))
        done

        if [[ $waited -ge $max_wait ]]; then
            warn "ChromaDB server did not respond within ${max_wait}s"
        fi

        success "Orchestrator started"
        echo ""
        daemon_status
    else
        error "Failed to start parent container"
    fi
}

daemon_stop() {
    info "Stopping HAL-9000 orchestrator..."

    # Check for running workers
    local worker_count
    worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')

    if [[ "$worker_count" -gt 0 ]]; then
        warn "There are $worker_count active workers"
        info "Workers will lose their network connection to ChromaDB"
        echo ""
        read -p "Stop anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Cancelled"
            return 0
        fi
    fi

    # Stop parent container
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Stopping parent container..."
        docker stop "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Orchestrator stopped"
    elif docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Container removed"
    else
        info "Orchestrator not running"
    fi
}

daemon_restart() {
    info "Restarting HAL-9000 orchestrator..."
    daemon_stop
    sleep 2
    daemon_start
}

handle_daemon_command() {
    local cmd="${1:-}"

    case "$cmd" in
        start)
            daemon_start
            ;;
        stop)
            daemon_stop
            ;;
        status)
            daemon_status
            ;;
        restart)
            daemon_restart
            ;;
        ""|help)
            echo "Usage: claudy daemon <command>"
            echo ""
            echo "Commands:"
            echo "  start    Start the orchestrator (parent container with ChromaDB server)"
            echo "  stop     Stop the orchestrator"
            echo "  status   Show orchestrator status and worker count"
            echo "  restart  Restart the orchestrator"
            ;;
        *)
            error "Unknown daemon command: $cmd"
            ;;
    esac
}

#==============================================================================
# Pool management
#==============================================================================

handle_pool_command() {
    local cmd="${1:-}"
    shift || true

    # Handle help without requiring parent
    if [[ "$cmd" == "help" ]] || [[ -z "$cmd" ]]; then
        echo "Usage: claudy pool <command> [options]"
        echo ""
        echo "Manage the worker pool for fast container startup."
        echo ""
        echo "Commands:"
        echo "  start [--min-warm N] [--max-warm N]  Start the pool manager"
        echo "  stop                                  Stop the pool manager"
        echo "  status                                Show pool status"
        echo "  scale <n>                             Scale to n warm workers"
        echo "  cleanup                               Force cleanup idle workers"
        echo "  warm                                  Create a single warm worker"
        echo ""
        echo "Options:"
        echo "  --min-warm N      Minimum warm workers (default: 2)"
        echo "  --max-warm N      Maximum warm workers (default: 5)"
        echo "  --idle-timeout N  Seconds before cleanup (default: 300)"
        return 0
    fi

    # Check if parent is running for other commands
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        error "Parent container not running. Start with: claudy daemon start"
    fi

    case "$cmd" in
        start)
            info "Starting pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh start "$@"
            ;;
        stop)
            info "Stopping pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh stop
            ;;
        status)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh status
            ;;
        scale)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh scale "$@"
            ;;
        cleanup)
            info "Cleaning up idle workers..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh cleanup
            ;;
        warm)
            info "Creating warm worker..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh warm
            ;;
        *)
            error "Unknown pool command: $cmd"
            ;;
    esac
}

#==============================================================================
# Via-parent launch (DinD mode)
#==============================================================================

launch_via_parent() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local detach="$6"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)

    # Check if parent is running
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        warn "Parent container not running"
        echo ""
        read -p "Start the orchestrator now? [Y/n] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Nn]$ ]]; then
            daemon_start
        else
            error "Cannot launch via parent without orchestrator. Run 'claudy daemon start' first."
        fi
    fi

    info "Launching worker via parent container..."
    info "  Worker: $session_name"
    info "  Profile: $profile"
    info "  Project: $abs_project_path"

    # Build spawn-worker command
    local spawn_args=()
    spawn_args+=(-n "$session_name")

    if [[ "$detach" == "true" ]]; then
        spawn_args+=(-d)
    fi

    # Use worker image based on profile
    local worker_image="${CONTAINER_IMAGE_BASE}:${profile}"
    spawn_args+=(-i "$worker_image")

    # Project directory - use HOST path because spawn-worker.sh
    # talks to the host's Docker daemon via socket
    spawn_args+=("$abs_project_path")

    # Execute spawn-worker.sh inside parent container
    # Pass through environment variables
    local exec_args=(docker exec)

    if [[ "$detach" != "true" ]]; then
        exec_args+=(-it)
    fi

    if [[ -n "$api_key" ]]; then
        exec_args+=(-e "ANTHROPIC_API_KEY=$api_key")
    elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        exec_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
    fi

    exec_args+=("$PARENT_CONTAINER" /scripts/spawn-worker.sh "${spawn_args[@]}")

    info "Executing: ${exec_args[*]}"

    if "${exec_args[@]}"; then
        if [[ "$detach" == "true" ]]; then
            success "Worker spawned in background: $session_name"
            info "Attach with: docker exec -it $session_name bash"
        fi
    else
        error "Failed to spawn worker via parent"
    fi
}

#==============================================================================
# Profile detection (See: claudy-installation-setup.md)
#==============================================================================

detect_profile() {
    local target_dir="${1:-.}"

    # Check for Java project
    if [[ -f "$target_dir/pom.xml" ]] || [[ -f "$target_dir/build.gradle" ]] || [[ -f "$target_dir/build.gradle.kts" ]]; then
        echo "java"
        return 0
    fi

    # Check for Python project
    if [[ -f "$target_dir/pyproject.toml" ]] || [[ -f "$target_dir/Pipfile" ]] || [[ -f "$target_dir/requirements.txt" ]]; then
        echo "python"
        return 0
    fi

    # Check for Node.js project
    if [[ -f "$target_dir/package.json" ]]; then
        echo "node"
        return 0
    fi

    # Default to base
    echo "$DEFAULT_PROFILE"
}

#==============================================================================
# Session management
#==============================================================================

get_session_name() {
    local project_dir="${1:-.}"
    local abs_path
    abs_path=$(cd "$project_dir" && pwd)

    # Hash the path to create deterministic session name
    # Use base64 to make it readable but unique
    local hash
    hash=$(echo -n "$abs_path" | shasum -a 256 | cut -c1-8)

    local basename
    basename=$(basename "$abs_path")

    # Clean basename for tmux (alphanumeric, hyphen, underscore only)
    basename=$(echo "$basename" | sed 's/[^a-zA-Z0-9_-]/-/g')

    echo "claudy-${basename}-${hash}"
}

verify_prerequisites() {
    local missing_tools=()

    # Check for required tools
    for tool in docker bash; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        fi
    done

    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        error "Missing required tools: ${missing_tools[*]}"
    fi

    # Check Docker daemon
    if ! docker ps &> /dev/null; then
        error "Docker daemon not running or not accessible"
    fi

    # Check authentication - required for container usage
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        echo ""
        warn "Authentication required for containerized Claude"
        echo ""
        info "Run the interactive setup:"
        echo ""
        echo "  claudy --setup"
        echo ""
        info "Or manually set your API key:"
        echo "  export ANTHROPIC_API_KEY=sk-ant-api03-..."
        echo "  Get key at: https://console.anthropic.com/settings/keys"
        echo ""
        error "Run 'claudy --setup' to configure authentication"
    fi

    success "Prerequisites verified"
}

#==============================================================================
# Session initialization
#==============================================================================

init_session() {
    local project_dir="${1:-.}"
    local profile="${2:-$(detect_profile "$project_dir")}"
    local session_name="${3:-$(get_session_name "$project_dir")}"
    local shell_mode="${4:-false}"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)

    # Create session directory structure
    local session_dir="$HAL9000_HOME/claude/$session_name"

    # Use SHARED config directory for auth persistence (login once, use everywhere)
    # Per-session isolation caused repeated login prompts
    local shared_claude_config="$HAL9000_HOME/claude/.shared-config"

    mkdir -p "$shared_claude_config"
    mkdir -p "$session_dir/.workspace"

    # Copy host Claude configuration to shared config (only on first setup)
    if [[ ! -f "$shared_claude_config/.initialized" ]]; then
        info "First-time setup: copying config to shared directory..."

        if [[ -d "$CLAUDE_HOME" ]]; then
            # Copy settings
            [[ -f "$CLAUDE_HOME/settings.json" ]] && cp "$CLAUDE_HOME/settings.json" "$shared_claude_config/"
            [[ -f "$CLAUDE_HOME/settings.local.json" ]] && cp "$CLAUDE_HOME/settings.local.json" "$shared_claude_config/"
            [[ -f "$CLAUDE_HOME/CLAUDE.md" ]] && cp "$CLAUDE_HOME/CLAUDE.md" "$shared_claude_config/"

            # Copy agents (exclude _shared directory which contains non-agent docs)
            if [[ -d "$CLAUDE_HOME/agents" ]]; then
                mkdir -p "$shared_claude_config/agents"
                find "$CLAUDE_HOME/agents" -maxdepth 1 -name "*.md" -exec cp {} "$shared_claude_config/agents/" \; 2>/dev/null || true
                local agent_count
                agent_count=$(find "$shared_claude_config/agents" -maxdepth 1 -name "*.md" 2>/dev/null | wc -l | tr -d ' ')
                info "Copied $agent_count agents"
            fi

            # Copy commands
            [[ -d "$CLAUDE_HOME/commands" ]] && cp -r "$CLAUDE_HOME/commands" "$shared_claude_config/" 2>/dev/null || true

            # Copy MCP server configurations
            if [[ -d "$CLAUDE_HOME/mcp-servers" ]]; then
                cp -r "$CLAUDE_HOME/mcp-servers" "$shared_claude_config/" 2>/dev/null || true
                local mcp_count
                mcp_count=$(find "$shared_claude_config/mcp-servers/servers" -name "*.json" 2>/dev/null | wc -l | tr -d ' ')
                info "Copied $mcp_count MCP server configs"
            fi

            # Copy skills
            [[ -d "$CLAUDE_HOME/skills" ]] && cp -r "$CLAUDE_HOME/skills" "$shared_claude_config/" 2>/dev/null || true
        fi

        # Mark as initialized
        touch "$shared_claude_config/.initialized"
        success "Shared config initialized"
    else
        info "Using existing shared config"
    fi

    # Create .claude.json with API key and MCP servers
    # This file tells Claude Code to use the API key directly (bypasses OAuth)
    # Also configures MCP servers that are pre-installed in the container
    local api_key_for_config="${ANTHROPIC_API_KEY:-}"
    if [[ -n "$api_key_for_config" ]]; then
        cat > "$shared_claude_config/.claude.json" << CLAUDEJSON
{
  "changelogLastFetched": 1000000000000,
  "primaryApiKey": "$api_key_for_config",
  "isQualifiedForDataSharing": false,
  "hasCompletedOnboarding": true,
  "lastOnboardingVersion": "2.1.19",
  "maxSubscriptionNoticeCount": 0,
  "hasAvailableMaxSubscription": false,
  "lastReleaseNotesSeen": "2.1.19",
  "mcpServers": {
    "allPepper-memory-bank": {
      "type": "stdio",
      "command": "mcp-server-memory-bank",
      "env": {
        "MEMORY_BANK_ROOT": "/root/memory-bank"
      }
    },
    "sequential-thinking": {
      "type": "stdio",
      "command": "mcp-server-sequential-thinking"
    },
    "chromadb": {
      "type": "stdio",
      "command": "chroma-mcp",
      "env": {
        "CHROMA_DB_PATH": "/root/chromadb"
      }
    }
  }
}
CLAUDEJSON
        chmod 600 "$shared_claude_config/.claude.json"
        info "Configured API key and MCP servers"
    fi

    # Create session metadata
    cat > "$session_dir/.claudy-session.json" << METADATA
{
  "name": "$session_name",
  "profile": "$profile",
  "project_dir": "$abs_project_path",
  "created_at": "$(date -Iseconds)",
  "shell_mode": $shell_mode,
  "container_image": "${CONTAINER_IMAGE_BASE}:${profile}",
  "claudy_version": "$SCRIPT_VERSION"
}
METADATA

    echo "$session_name"
}

#==============================================================================
# Container launching
#==============================================================================

launch_container_session() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)
    local session_dir="$HAL9000_HOME/claude/$session_name"
    local shared_claude_config="$HAL9000_HOME/claude/.shared-config"
    local container_image="${CONTAINER_IMAGE_BASE}:${profile}"

    # Verify image exists or fall back to base
    if ! docker inspect "$container_image" &> /dev/null; then
        warn "Image not found: $container_image, using base profile"
        container_image="${CONTAINER_IMAGE_BASE}:base"
    fi

    info "Launching Claude in container..."
    info "  Profile: $profile"
    info "  Session: $session_name"
    info "  Directory: $abs_project_path"

    # Build docker run command
    # Key technique from claude-container: use CLAUDE_CONFIG_DIR instead of HOME manipulation
    # Use SHARED config so login persists across all sessions
    # Also pass USER_UID/USER_GID for proper file ownership
    local docker_args=(
        docker run
        --rm
        -it
        --name "$session_name"
        -v "$abs_project_path:/workspace"
        -v "$shared_claude_config:/claude"
        -v /var/run/docker.sock:/var/run/docker.sock
        -e "CLAUDE_CONFIG_DIR=/claude"
        -e "USER_UID=$(id -u)"
        -e "USER_GID=$(id -g)"
        -e "CLAUDY_SESSION=$session_name"
        -e "CLAUDY_PROJECT_DIR=$abs_project_path"
        -e "MEMORY_BANK_ROOT=/home/claude/memory-bank"
    )

    # Pass through API key if available
    if [[ -n "$api_key" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$api_key")
        info "  Auth: API key"
    elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
        info "  Auth: API key (from environment)"
    else
        info "  Auth: Interactive login required"
    fi

    # Add image
    docker_args+=(-w "/workspace" "$container_image")

    # Run shell or Claude (using tmux inside container for session persistence)
    # CLAUDE_CONFIG_DIR tells Claude where to find config, no HOME manipulation needed
    if [[ "$shell_mode" == "true" ]]; then
        info "Starting bash shell in tmux..."
        "${docker_args[@]}" tmux new-session -s shell
    else
        info "Starting Claude CLI in tmux..."
        "${docker_args[@]}" tmux new-session -s claude "claude"
    fi
}

#==============================================================================
# Main entry point
#==============================================================================

main() {
    local project_dir="."
    local profile=""
    local session_name=""
    local claude_home="${CLAUDE_HOME:-$DEFAULT_CLAUDE_HOME}"
    local api_key=""
    local shell_mode=false
    local detach=false
    local verify_only=false
    local via_parent=false
    local legacy_mode=false

    # Handle daemon subcommand first
    if [[ "${1:-}" == "daemon" ]]; then
        shift
        handle_daemon_command "$@"
        exit 0
    fi

    # Handle pool subcommand
    if [[ "${1:-}" == "pool" ]]; then
        shift
        handle_pool_command "$@"
        exit 0
    fi

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --profile)
                profile="$2"
                shift 2
                ;;
            --shell|-s)
                shell_mode=true
                shift
                ;;
            --name)
                session_name="$2"
                shift 2
                ;;
            --claude-home)
                claude_home="$2"
                shift 2
                ;;
            --api-key)
                api_key="$2"
                shift 2
                ;;
            --detach|-d)
                detach=true
                shift
                ;;
            --via-parent)
                via_parent=true
                shift
                ;;
            --legacy)
                legacy_mode=true
                shift
                ;;
            --verify)
                verify_only=true
                shift
                ;;
            --setup)
                setup_auth
                exit 0
                ;;
            --diagnose)
                export CLAUDE_HOME="$claude_home"
                show_diagnostics
                exit 0
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            --version|-v)
                show_version
                exit 0
                ;;
            -*)
                error "Unknown option: $1"
                ;;
            *)
                project_dir="$1"
                shift
                ;;
        esac
    done

    # Export CLAUDE_HOME for use in functions
    export CLAUDE_HOME="$claude_home"

    # Handle legacy mode (deprecated single-container mode)
    if [[ "$legacy_mode" == "true" ]]; then
        warn "DEPRECATION: --legacy mode is deprecated and will be removed in v1.0"
        warn "Please migrate to DinD architecture:"
        warn "  1. Run: ./scripts/migrate-to-dind.sh"
        warn "  2. Use: claudy daemon start && claudy --via-parent"
        echo ""
        if [[ "$via_parent" == "true" ]]; then
            warn "--legacy overrides --via-parent; using single-container mode"
            via_parent=false
        fi
    fi

    # Verify prerequisites (skip API key check for via-parent mode)
    if [[ "$via_parent" == "true" ]]; then
        # For via-parent, just check Docker is available
        if ! command -v docker &> /dev/null; then
            error "Docker not found"
        fi
        if ! docker ps &> /dev/null; then
            error "Docker daemon not running"
        fi
    else
        if ! verify_prerequisites; then
            exit 1
        fi
    fi

    if [[ "$verify_only" == "true" ]]; then
        exit 0
    fi

    # Change to project directory
    if [[ ! -d "$project_dir" ]]; then
        error "Directory not found: $project_dir"
    fi

    # Auto-detect profile if not specified
    if [[ -z "$profile" ]]; then
        profile=$(detect_profile "$project_dir")
        info "Auto-detected profile: $profile"
    fi

    # Initialize session
    if [[ -z "$session_name" ]]; then
        session_name=$(get_session_name "$project_dir")
    fi

    # Launch via parent or direct
    if [[ "$via_parent" == "true" ]]; then
        # DinD mode: spawn worker via parent container
        info "Using via-parent mode (DinD)"
        launch_via_parent "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" "$detach" || error "Failed to launch via parent"
    else
        # Direct mode: launch container directly
        info "Initializing session..."
        init_session "$project_dir" "$profile" "$session_name" "$shell_mode" || error "Failed to initialize session"

        success "Session ready: $session_name"

        # Launch container
        launch_container_session "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" || error "Failed to launch container"
    fi
}

# Run main if script is executed directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
