#!/bin/bash
# hal-9000 - Containerized Claude CLI launcher
# Opens Claude inside a tmux session within a hal-9000 container

set -Eeuo pipefail

readonly SCRIPT_NAME="hal-9000"
readonly SCRIPT_VERSION="2.1.0"
readonly HAL9000_HOME="${HOME}/.hal9000"
readonly HAL9000_PROFILES_DIR="${HAL9000_HOME}/profiles"
readonly HAL9000_CONFIG_FILE="${HAL9000_HOME}/config"
readonly HAL9000_SECRETS_DIR="${HAL9000_HOME}/secrets"
readonly PARENT_CONTAINER="hal9000-parent"
readonly PARENT_IMAGE="ghcr.io/hellblazer/hal-9000:parent"
readonly DEFAULT_PROFILE="base"
readonly DEFAULT_CONTAINER_IMAGE_BASE="ghcr.io/hellblazer/hal-9000"

# Base volume names (will be suffixed with user hash for isolation)
readonly CLAUDE_HOME_VOLUME_BASE="hal9000-claude-home"
readonly CLAUDE_SESSION_VOLUME_BASE="hal9000-claude-session"
readonly MEMORY_BANK_VOLUME_BASE="hal9000-memory-bank"

# Cached user hash for consistent volume naming within a session
_CACHED_USER_HASH=""

# Claude subcommands that should be passed through to container
readonly CLAUDE_SUBCOMMANDS="mcp|plugin|doctor|install|setup-token|update"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

#==============================================================================
# Error handling
#==============================================================================

error() {
    local exit_code="${2:-1}"
    echo -e "${RED}✗ Error: $1${NC}" >&2
    exit "$exit_code"
}

warn() {
    echo -e "${YELLOW}⚠ Warning: $*${NC}" >&2
}

info() {
    echo -e "${BLUE}ℹ $*${NC}"
}

success() {
    echo -e "${GREEN}✓ $*${NC}"
}

#==============================================================================
# Configuration and local profiles
#==============================================================================

# Validate profile name (prevent path traversal attacks)
validate_profile_name() {
    local name="$1"

    # Only allow alphanumeric, dash, underscore
    if [[ ! "$name" =~ ^[a-zA-Z0-9_-]+$ ]]; then
        return 1
    fi

    # Additional check: reject common path traversal patterns
    if [[ "$name" == ".."* ]] || [[ "$name" == *".."* ]]; then
        return 1
    fi

    return 0
}

# MEDIUM-2: Validate container image format
# Ensures CONTAINER_IMAGE_BASE matches expected Docker image format
validate_container_image_base() {
    local image="$1"
    # Pattern: registry/path:tag or path:tag
    # Allows: lowercase alphanumeric, dots, dashes, underscores, slashes, colons
    # Format: [registry/]name[:tag]
    if [[ ! "$image" =~ ^[a-z0-9][a-z0-9._/-]*:[a-z0-9._-]+$ ]] && \
       [[ ! "$image" =~ ^[a-z0-9][a-z0-9._/-]*$ ]]; then
        error "Invalid CONTAINER_IMAGE_BASE format: $image"
    fi
    return 0
}

# Load configuration from ~/.hal9000/config (safe parsing, no arbitrary code execution)
load_config() {
    if [[ ! -f "$HAL9000_CONFIG_FILE" ]]; then
        return 0
    fi

    # Safely parse config file - only allow VAR=value assignments with known variables
    while IFS='=' read -r key value; do
        # Skip empty lines and comments
        [[ -z "$key" ]] && continue
        [[ "$key" =~ ^[[:space:]]*# ]] && continue

        # Trim whitespace
        key=$(echo "$key" | xargs)
        value=$(echo "$value" | xargs)

        # Only allow specific known variables (whitelist approach)
        case "$key" in
            CONTAINER_IMAGE_BASE)
                # MEDIUM-2: Validate container image format before accepting
                validate_container_image_base "$value"
                export CONTAINER_IMAGE_BASE="$value"
                ;;
            *)
                # SECURITY: Log warning for unknown config variables
                # Helps detect typos or accidental configuration changes
                # Known variables: CONTAINER_IMAGE_BASE
                warn "Config: Unknown variable '$key' in $HAL9000_CONFIG_FILE (ignored for security)"
                ;;
        esac
    done < "$HAL9000_CONFIG_FILE"
}

#==============================================================================
# Secrets management
#==============================================================================

# Initialize secrets directory with secure permissions
init_secrets_dir() {
    if [[ ! -d "$HAL9000_SECRETS_DIR" ]]; then
        mkdir -p "$HAL9000_SECRETS_DIR"
        chmod 700 "$HAL9000_SECRETS_DIR"
    fi
}

# SECURITY: Check for API key in environment and fail with instructions
# API keys should NEVER be passed via environment variables as they are visible
# in docker inspect output and /proc/1/environ inside containers.
# Usage: check_api_key_security
check_api_key_security() {
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        error "SECURITY VIOLATION: ANTHROPIC_API_KEY environment variable detected.

API keys in environment variables are a security risk:
  - Visible via 'docker inspect' on container
  - Visible via /proc/1/environ inside container
  - May be logged in shell history

For security, API keys MUST be stored in secret files, not environment variables.

To fix:
  1. Store your key securely:
     echo 'your-api-key' > ${HAL9000_SECRETS_DIR}/anthropic_key
     chmod 400 ${HAL9000_SECRETS_DIR}/anthropic_key

  2. Remove the environment variable:
     unset ANTHROPIC_API_KEY

  3. Update your shell profile (~/.bashrc, ~/.zshrc) to NOT set this variable

  4. Use subscription login instead (recommended):
     hal-9000 /login" 1
    fi
}

# Legacy function - now just calls security check
# Kept for backward compatibility with scripts that call this function
store_api_key_secret() {
    check_api_key_security
}

# Initialize seccomp profile on host filesystem
# Docker requires the seccomp profile to be on the HOST before container starts
# This copies the profile from the plugin directory to HAL9000_HOME
init_seccomp_profile() {
    local seccomp_dir="${HAL9000_HOME}/seccomp"
    local seccomp_profile="${seccomp_dir}/hal9000.json"

    # Source profile location (bundled with hal-9000)
    # Try multiple locations for the source profile
    local source_profile=""
    local search_paths=(
        "${HAL9000_HOME}/../hal-9000/plugins/hal-9000/seccomp/hal9000.json"
        "/opt/hal-9000/seccomp/hal9000.json"
        "$(dirname "${BASH_SOURCE[0]}")/plugins/hal-9000/seccomp/hal9000.json"
    )

    for path in "${search_paths[@]}"; do
        if [[ -f "$path" ]]; then
            source_profile="$path"
            break
        fi
    done

    # Create seccomp directory
    mkdir -p "$seccomp_dir"
    chmod 755 "$seccomp_dir"

    if [[ -n "$source_profile" ]]; then
        # Copy profile to HAL9000_HOME
        cp "$source_profile" "$seccomp_profile"
        chmod 644 "$seccomp_profile"
        success "Seccomp profile initialized: $seccomp_profile"
    elif [[ ! -f "$seccomp_profile" ]]; then
        # MEDIUM-10: Fail if seccomp profile is unavailable (fail-closed security)
        # Do not create minimal profile - require proper security configuration
        error "SECURITY: Seccomp profile not found and no source available.
Seccomp profiles are required for container security.

To fix:
  1. Clone the hal-9000 repository to get the seccomp profile
  2. Or manually create: $seccomp_profile

Searched locations:
$(for p in "${search_paths[@]}"; do echo "  - $p"; done)

This is a security requirement - containers will not start without a seccomp profile." 1
    else
        info "Seccomp profile already exists: $seccomp_profile"
    fi
}

# Get path to API key secret file (if it exists)
# Returns: path to secret file, or empty if not available
get_api_key_secret_path() {
    local secret_file="${HAL9000_SECRETS_DIR}/anthropic_key"
    if [[ -f "$secret_file" ]]; then
        echo "$secret_file"
    fi
}

# Check if API key is available (secret file only - env vars are blocked for security)
has_api_key() {
    if [[ -f "${HAL9000_SECRETS_DIR}/anthropic_key" ]]; then
        return 0
    fi
    return 1
}

#==============================================================================
# User identity and volume isolation
#==============================================================================

# SECURITY: Per-user volume isolation
# Each user gets their own isolated volumes to prevent cross-user attacks:
# - Malicious plugin installation affecting other users
# - Memory bank poisoning across users
# - Session state tampering

# Generate user hash for volume isolation
# Uses $USER if available, falls back to UID for containerized environments
generate_user_hash() {
    local user_id

    # Prefer $USER environment variable (human-readable, stable across sessions)
    if [[ -n "${USER:-}" ]]; then
        user_id="$USER"
    # Fall back to username from id command
    elif user_id=$(id -un 2>/dev/null); then
        : # user_id already set
    # Last resort: use numeric UID
    else
        user_id="uid-$(id -u 2>/dev/null || echo "unknown")"
    fi

    # Generate 8-character hash for volume naming
    # SHA-256 ensures consistent, collision-resistant hash
    if command -v sha256sum &>/dev/null; then
        echo -n "$user_id" | sha256sum | cut -c1-8
    elif command -v shasum &>/dev/null; then
        # macOS uses shasum instead of sha256sum
        echo -n "$user_id" | shasum -a 256 | cut -c1-8
    else
        # Fallback: use first 8 chars of user_id
        echo "${user_id:0:8}"
    fi
}

# Get cached user hash (computed once, reused for consistency)
get_user_hash() {
    if [[ -z "$_CACHED_USER_HASH" ]]; then
        _CACHED_USER_HASH=$(generate_user_hash)
    fi
    echo "$_CACHED_USER_HASH"
}

# Get user-scoped volume name
# Usage: get_user_volume <base-name>
# Example: get_user_volume "claude-home" -> "hal9000-claude-home-a1b2c3d4"
get_user_volume() {
    local base_name="$1"
    local user_hash
    user_hash=$(get_user_hash)
    echo "hal9000-${base_name}-${user_hash}"
}

# Get user-scoped CLAUDE_HOME volume name
get_claude_home_volume() {
    get_user_volume "claude-home"
}

# Get user-scoped session volume name
get_claude_session_volume() {
    get_user_volume "claude-session"
}

# Get user-scoped memory bank volume name
get_memory_bank_volume() {
    get_user_volume "memory-bank"
}

# Get container image base with precedence: env var > config file > default
get_container_image_base() {
    # Environment variable takes highest precedence
    if [[ -n "${HAL9000_CONTAINER_IMAGE_BASE:-}" ]]; then
        echo "$HAL9000_CONTAINER_IMAGE_BASE"
        return 0
    fi

    # Load and check config file
    load_config
    if [[ -n "${CONTAINER_IMAGE_BASE:-}" ]]; then
        echo "$CONTAINER_IMAGE_BASE"
        return 0
    fi

    # Default
    echo "$DEFAULT_CONTAINER_IMAGE_BASE"
}

# Check if a local profile exists and optionally build it
get_local_profile_image() {
    local profile="$1"

    # Validate profile name (prevent path traversal)
    if ! validate_profile_name "$profile"; then
        error "Invalid profile name: $profile (only alphanumeric, dash, underscore allowed)" 2
    fi

    local local_profile_dir="${HAL9000_PROFILES_DIR}/${profile}"

    if [[ -f "${local_profile_dir}/Dockerfile" ]]; then
        # Local profile exists - return the image name
        echo "hal-9000-local-${profile}:latest"
        return 0
    fi

    # No local profile
    return 1
}

# Build a local profile if it exists and is not already built
build_local_profile() {
    local profile="$1"

    # Validate profile name (prevent path traversal)
    if ! validate_profile_name "$profile"; then
        error "Invalid profile name: $profile (only alphanumeric, dash, underscore allowed)" 2
    fi

    local local_profile_dir="${HAL9000_PROFILES_DIR}/${profile}"

    if [[ ! -f "${local_profile_dir}/Dockerfile" ]]; then
        # No local profile to build
        return 0
    fi

    local image_name="hal-9000-local-${profile}:latest"

    # Check if already built
    if docker inspect "$image_name" &>/dev/null; then
        return 0
    fi

    # Build the local profile
    info "Building local profile: $profile"
    if ! docker build -f "${local_profile_dir}/Dockerfile" -t "$image_name" "$local_profile_dir" >/dev/null 2>&1; then
        error "Failed to build local profile: $profile" 3
    fi
    success "Local profile built: $profile"
}

# List available local profiles
list_local_profiles() {
    if [[ ! -d "$HAL9000_PROFILES_DIR" ]]; then
        return 0
    fi

    echo ""
    echo "Local profiles available in $HAL9000_PROFILES_DIR:"
    for profile_dir in "$HAL9000_PROFILES_DIR"/*; do
        if [[ -d "$profile_dir" && -f "${profile_dir}/Dockerfile" ]]; then
            local profile_name=$(basename "$profile_dir")
            local image_name="hal-9000-local-${profile_name}:latest"

            if docker inspect "$image_name" &>/dev/null; then
                echo "  ✓ $profile_name (built)"
            else
                echo "  ○ $profile_name (not yet built)"
            fi
        fi
    done
    echo ""
}

#==============================================================================
# Claude command passthrough
#==============================================================================

# Ensure user-scoped volumes exist
ensure_volumes() {
    # SECURITY: Use user-scoped volumes for isolation
    local claude_home_vol
    local claude_session_vol
    local memory_bank_vol
    claude_home_vol=$(get_claude_home_volume)
    claude_session_vol=$(get_claude_session_volume)
    memory_bank_vol=$(get_memory_bank_volume)

    docker volume create "$claude_home_vol" >/dev/null 2>&1 || true
    docker volume create "$claude_session_vol" >/dev/null 2>&1 || true
    docker volume create "$memory_bank_vol" >/dev/null 2>&1 || true

    # Initialize seccomp profile on host filesystem (required for direct mode)
    # This ensures the profile is available before any container starts
    init_seccomp_profile 2>/dev/null || true

    # Initialize volumes with pristine Claude config on first creation
    # This ensures containers start in a clean state with no host-specific paths
    docker run --rm \
        -v "${claude_session_vol}:/session" \
        -v "${claude_home_vol}:/claude-home" \
        alpine:latest \
        sh -c '
            # Initialize session volume with pristine Claude config
            if [ ! -f /session/.initialized ]; then
                cat > /session/.claude.json << '"'"'EOF'"'"'
{
  "theme": "auto",
  "installMethod": "native",
  "autoUpdates": false
}
EOF
                touch /session/.initialized
            fi

            # Initialize CLAUDE_HOME structure if empty
            if [ ! -d /claude-home/plugins ]; then
                mkdir -p /claude-home/plugins
                touch /claude-home/.initialized
            fi
        ' >/dev/null 2>&1 || true
}

# Check if subscription login is configured in the user-scoped volume
# Returns 0 if logged in, 1 if not
check_subscription_auth() {
    ensure_volumes

    local claude_home_vol
    claude_home_vol=$(get_claude_home_volume)

    # Check for auth credentials in the volume
    # Claude stores session data in .claude/
    docker run --rm \
        -v "${claude_home_vol}:/root/.claude:ro" \
        alpine:latest \
        sh -c 'test -f /root/.claude/.credentials.json || test -f /root/.claude/statsig_user_id' \
        2>/dev/null
}

# Run a claude command inside a container with shared volumes
# Usage: run_claude_command <args...>
run_claude_command() {
    ensure_volumes

    # Store API key to secret file if available (security: avoid env var exposure)
    store_api_key_secret

    local image_base
    image_base=$(get_container_image_base)
    local image="${image_base}:base"

    # SECURITY: Use user-scoped volumes for isolation
    local claude_home_vol
    local claude_session_vol
    local memory_bank_vol
    claude_home_vol=$(get_claude_home_volume)
    claude_session_vol=$(get_claude_session_volume)
    memory_bank_vol=$(get_memory_bank_volume)

    # Build docker args
    local docker_args=(
        docker run --rm -it
        -v "${claude_home_vol}:/root/.claude"
        -v "${claude_session_vol}:/root/.claude-session"
        -v "${memory_bank_vol}:/root/memory-bank"
        -e "CLAUDE_HOME=/root/.claude"
        -e "MEMORY_BANK_ROOT=/root/memory-bank"
    )

    # SECURITY: Mount API key as secret file instead of environment variable
    # This prevents exposure via 'docker inspect' or /proc/1/environ
    local secret_path
    secret_path=$(get_api_key_secret_path)
    if [[ -n "$secret_path" ]]; then
        docker_args+=(-v "${secret_path}:/run/secrets/anthropic_key:ro")
    fi

    # Run claude with session state restoration and persistence
    # This ensures commands like /login save credentials properly
    "${docker_args[@]}" "$image" bash -c '
        # Restore session state from persistent volume
        if [ -f /root/.claude-session/.claude.json ]; then
          cp /root/.claude-session/.claude.json /root/.claude.json
        fi
        # SECURITY: Read API key only at command execution time
        # Using VAR=value command syntax keeps the key in claude process only, not exported to shell
        if [ -f /run/secrets/anthropic_key ]; then
          ANTHROPIC_API_KEY=$(cat /run/secrets/anthropic_key) claude "$@"
        else
          claude "$@"
        fi
        # Save session state back to persistent volume
        [ -f /root/.claude.json ] && cp /root/.claude.json /root/.claude-session/.claude.json
    ' bash "$@"
}

# Check if arg is a claude subcommand that should be passed through
is_claude_subcommand() {
    local arg="${1:-}"
    # Match known subcommands (mcp, plugin, doctor, etc.)
    [[ "$arg" =~ ^($CLAUDE_SUBCOMMANDS)$ ]] && return 0
    # Match Claude slash commands like /login, /help, /status
    [[ "$arg" =~ ^/ ]] && [[ ! -d "$arg" ]] && return 0
    return 1
}

#==============================================================================
# Help and diagnostics
#==============================================================================

show_help() {
    cat << 'EOF'
hal-9000 - Containerized Claude CLI

USAGE:
  hal-9000 [OPTIONS] [DIRECTORY]           # Interactive Claude session
  hal-9000 <claude-command> [args...]      # Run claude command in container
  hal-9000 daemon <command>                # Manage orchestrator
  hal-9000 pool <command>                  # Manage worker pool
  hal-9000 sessions                        # List running sessions
  hal-9000 attach [session]                # Attach to existing session
  hal-9000 kill <session>                  # Stop a session

CLAUDE COMMANDS (passthrough to container):
  hal-9000 plugin list                     # List installed plugins
  hal-9000 plugin install <name>           # Install a plugin
  hal-9000 plugin marketplace add <url>    # Add a marketplace
  hal-9000 mcp list                        # List MCP servers
  hal-9000 mcp add <server>                # Add MCP server
  hal-9000 doctor                          # Check Claude health

  All 'claude' subcommands work - they run inside a container with the
  shared CLAUDE_HOME volume, so changes persist across all sessions.

DAEMON COMMANDS:
  hal-9000 daemon start    Start the orchestrator (parent container with ChromaDB)
  hal-9000 daemon stop     Stop the orchestrator
  hal-9000 daemon status   Show orchestrator status and worker count
  hal-9000 daemon restart  Restart the orchestrator

POOL COMMANDS:
  hal-9000 pool start      Start the worker pool manager
  hal-9000 pool stop       Stop the worker pool manager
  hal-9000 pool status     Show pool status (warm/busy workers)
  hal-9000 pool scale <n>  Scale to n warm workers
  hal-9000 pool cleanup    Force cleanup of idle workers

SESSION COMMANDS:
  hal-9000 sessions        List running hal-9000 sessions
  hal-9000 attach [name]   Reattach to an existing session (auto-selects if only one)
  hal-9000 kill <name>     Stop and remove a session

  Sessions persist after exiting. To reattach to a running container,
  use 'hal-9000 attach' and the container will drop you into a bash shell.

OPTIONS:
  --setup                Interactive setup (DEPRECATED - use secret file instead)
  --profile, -p PROFILE  Profile: base, python, node, java (default: auto-detect)
  --shell, -s            Start bash shell instead of Claude
  --name NAME            Custom session name (default: auto from directory)
  --detach, -d           Don't attach to tmux session after launch
  --via-parent           Launch worker via parent container (requires daemon running)
  --verify               Verify prerequisites and exit
  --diagnose             Show diagnostic information
  --help, -h             Show this help message
  --version, -v          Show version

ARGUMENTS:
  DIRECTORY              Project directory (default: current directory)

AUTHENTICATION:
  Option 1 - Subscription Login (recommended):
    hal-9000 /login      # Login once, persists for all sessions

  Option 2 - API Key (via secret file):
    echo 'sk-ant-api03-...' > ~/.hal9000/secrets/anthropic_key
    chmod 400 ~/.hal9000/secrets/anthropic_key

    Get key at: https://console.anthropic.com/settings/keys

  SECURITY NOTE: Environment variables (ANTHROPIC_API_KEY) are NOT supported.
  Environment variables are visible via 'docker inspect' and /proc/1/environ.
  Always use file-based secrets for API keys.

  Auth is stored in shared Docker volume - login once, use everywhere.

EXAMPLES:
  hal-9000                                    # Current dir, auto-detect profile
  hal-9000 --profile python                   # Force Python profile
  hal-9000 ~/projects/myapp                   # Specific directory
  hal-9000 --shell                            # Start with shell instead of Claude
  hal-9000 plugin install memory-bank         # Install plugin (persists)
  hal-9000 mcp list                           # List MCP servers
  hal-9000 --diagnose                         # Check setup

SESSION EXAMPLES:
  hal-9000 ~/project                          # Start new session, launches Claude
  # Inside Claude: type 'exit' to return to bash shell
  hal-9000 sessions                           # List all running sessions
  hal-9000 attach                             # Reattach to a session (auto-pick if one)
  hal-9000 attach hal-9000-myapp-abc123       # Reattach to specific session
  hal-9000 kill hal-9000-myapp-abc123         # Stop a session

DAEMON EXAMPLES:
  hal-9000 daemon start                       # Start the orchestrator
  hal-9000 daemon status                      # Check status and workers
  hal-9000 daemon stop                        # Stop the orchestrator
  hal-9000 --via-parent ~/project             # Launch worker via parent (DinD mode)

PROFILE COMMANDS:
  hal-9000 profiles                           # List available local profiles
  hal-9000 profiles list                      # List available local profiles (same as above)
  hal-9000 profiles build <name>              # Build a specific local profile

CUSTOM PROFILES:
  Create profiles in ~/.hal9000/profiles/:
    mkdir -p ~/.hal9000/profiles/ruby
    cat > ~/.hal9000/profiles/ruby/Dockerfile << 'EOF'
    FROM ghcr.io/hellblazer/hal-9000:base
    RUN gem install rails bundler
    EOF

  Then hal-9000 automatically detects and uses it:
    cd /path/to/ruby/project
    hal-9000

  See README-CUSTOM_PROFILES.md for complete guide.

ENVIRONMENT:
  HAL9000_CONTAINER_IMAGE_BASE   Override Docker image registry/base
                                 (e.g., "my-registry/my-hal-9000")
  HAL9000_HOME                   Override session storage (default: ~/.hal9000)
  DOCKER_SOCKET                  Docker socket path (default: /var/run/docker.sock)

  DEPRECATED (security risk - do not use):
  ANTHROPIC_API_KEY              Use ~/.hal9000/secrets/anthropic_key file instead

CONFIGURATION:
  Config file: ~/.hal9000/config
  Set persistent defaults (e.g., custom image registry):
    cat > ~/.hal9000/config << 'EOF'
    CONTAINER_IMAGE_BASE=my-registry/hal-9000
    EOF

  Precedence: environment variable > config file > default

VOLUMES (User-Isolated):
  Volumes are isolated per user for security. Each user gets their own volumes:

  hal9000-claude-home-<hash>       User's CLAUDE_HOME - plugins, credentials, config
  hal9000-claude-session-<hash>    User's session state - .claude.json (auth state)
  hal9000-memory-bank-<hash>       User's memory bank for cross-session context

  Where <hash> is an 8-character hash of your username/UID.
  This prevents cross-user attacks (malicious plugin installation, data poisoning).

For more information, see documentation at:
  https://github.com/hellblazer/hal-9000/blob/main/README.md
EOF
}

show_version() {
    echo "hal-9000 version $SCRIPT_VERSION"
    echo "hal-9000 plugin version $SCRIPT_VERSION"
}

setup_auth() {
    echo ""
    info "hal-9000 Secure Authentication Setup"
    echo ""

    local secret_file="${HAL9000_SECRETS_DIR}/anthropic_key"

    # Check if already configured via secret file
    if [[ -f "$secret_file" ]]; then
        success "API key secret file already exists: $secret_file"
        echo ""
        read -p "Replace with a new key? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Keeping existing configuration"
            exit 0
        fi
    fi

    # Check for insecure env var and warn
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        warn "ANTHROPIC_API_KEY environment variable detected!"
        warn "Environment variables are a security risk (visible via docker inspect)."
        warn "This setup will migrate your key to a secure file."
        echo ""
    fi

    echo "Enter your Anthropic API key (or 'q' to quit):"
    echo "  Get one at: https://console.anthropic.com/settings/keys"
    echo ""
    read -p "API Key: " -s api_key_input  # -s hides input for security
    echo ""

    if [[ "$api_key_input" == "q" ]] || [[ -z "$api_key_input" ]]; then
        info "Setup cancelled"
        exit 0
    fi

    # Validate format
    if [[ ! "$api_key_input" =~ ^sk-ant- ]]; then
        warn "Key doesn't look like an Anthropic API key (expected sk-ant-...)"
        read -p "Continue anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi

    # Create secrets directory and store key securely
    init_secrets_dir
    (umask 077 && echo "$api_key_input" > "$secret_file")
    chmod 400 "$secret_file"

    success "API key stored securely in: $secret_file"
    echo ""
    info "File permissions: 400 (owner read-only)"
    echo ""

    # If env var was set, remind user to remove it
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        warn "IMPORTANT: Remove ANTHROPIC_API_KEY from your shell profile!"
        echo ""
        echo "  1. Edit your shell config (~/.bashrc, ~/.zshrc, etc.)"
        echo "  2. Remove any line containing: export ANTHROPIC_API_KEY"
        echo "  3. Run: unset ANTHROPIC_API_KEY"
        echo ""
    fi

    success "Setup complete! You can now run: hal-9000"
}

show_diagnostics() {
    info "hal-9000 Diagnostics"
    echo ""

    # Check Docker
    if command -v docker &> /dev/null; then
        DOCKER_VERSION=$(docker --version)
        success "Docker: $DOCKER_VERSION"
    else
        error "Docker not found. Install Docker to use hal-9000." 3
    fi

    # Check tmux
    if command -v tmux &> /dev/null; then
        TMUX_VERSION=$(tmux -V)
        success "tmux: $TMUX_VERSION"
    else
        warn "tmux not found. Install tmux for full functionality."
    fi

    # Check bash version
    BASH_VERSION="${BASH_VERSION%.*}"
    if [[ "${BASH_VERSION}" =~ ^[5-9]|[0-9]{2,} ]]; then
        success "Bash: $BASH_VERSION (OK)"
    else
        warn "Bash version: $BASH_VERSION (recommend 5.0+)"
    fi

    # Check authentication
    echo ""
    info "Authentication:"

    local secret_file="${HAL9000_SECRETS_DIR}/anthropic_key"
    if [[ -f "$secret_file" ]]; then
        success "  API Key: Secure file exists ($secret_file)"
    else
        info "  API Key: No secret file"
    fi

    # Warn about insecure env var
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        warn "  SECURITY: ANTHROPIC_API_KEY env var detected (insecure!)"
        warn "    → Move to: $secret_file"
        warn "    → Run: unset ANTHROPIC_API_KEY"
    fi

    if check_subscription_auth; then
        success "  Subscription: Logged in (credentials in volume)"
    else
        info "  Subscription: Not logged in"
        info "    → Run 'hal-9000 /login' to authenticate"
    fi

    # Check Docker volumes (user-scoped for isolation)
    echo ""
    local user_hash
    user_hash=$(get_user_hash)
    info "Docker Volumes (user: ${USER:-$(id -un 2>/dev/null || echo uid-$(id -u))}, hash: ${user_hash}):"

    local claude_home_vol
    local memory_bank_vol
    claude_home_vol=$(get_claude_home_volume)
    memory_bank_vol=$(get_memory_bank_volume)

    if docker volume inspect "$claude_home_vol" &>/dev/null; then
        success "  $claude_home_vol (exists)"
        # Show volume size if possible
        local vol_path
        vol_path=$(docker volume inspect "$claude_home_vol" --format '{{.Mountpoint}}' 2>/dev/null || true)
        if [[ -n "$vol_path" ]]; then
            info "    → Mountpoint: $vol_path"
        fi
    else
        info "  $claude_home_vol (not yet created)"
    fi

    if docker volume inspect "$memory_bank_vol" &>/dev/null; then
        success "  $memory_bank_vol (exists)"
    else
        info "  $memory_bank_vol (not yet created)"
    fi

    # Check hal9000 storage
    echo ""
    if [[ -d "$HAL9000_HOME" ]]; then
        SESSION_COUNT=$(find "$HAL9000_HOME/claude" -maxdepth 1 -type d 2>/dev/null | wc -l)
        success "hal9000 home: $HAL9000_HOME"
        info "  → Active sessions: $((SESSION_COUNT - 1))"
    else
        info "hal9000 home: $HAL9000_HOME (not yet created)"
    fi

    # Check Docker socket
    if [[ -S /var/run/docker.sock ]]; then
        success "Docker socket: /var/run/docker.sock"
    elif [[ -S "$HOME/.docker/run/docker.sock" ]]; then
        success "Docker socket: $HOME/.docker/run/docker.sock"
    else
        warn "Docker socket not found"
    fi

    # Check for container image
    echo ""
    local image_base
    image_base=$(get_container_image_base)
    if docker inspect "${image_base}:base" &> /dev/null; then
        success "Container image: ${image_base}:base (available)"
    else
        warn "Container image: ${image_base}:base (not found - run: make build-base)"
    fi

    echo ""
    info "System: $(uname -s) $(uname -m)"
    info "Current directory: $(pwd)"

    # Auth guidance
    echo ""
    local secret_file="${HAL9000_SECRETS_DIR}/anthropic_key"
    if [[ ! -f "$secret_file" ]] && ! check_subscription_auth; then
        info "Authentication options:"
        echo "  1. Subscription login (recommended): hal-9000 /login"
        echo "  2. API key via secure file:"
        echo "     echo 'sk-ant-...' > ~/.hal9000/secrets/anthropic_key"
        echo "     chmod 400 ~/.hal9000/secrets/anthropic_key"
        echo ""
        warn "Note: Environment variables (ANTHROPIC_API_KEY) are NOT supported"
        echo "      Environment variables are visible via 'docker inspect'"
    fi

    # Container config info
    echo ""
    info "Container config:"
    echo "  CLAUDE_HOME: /root/.claude (from $claude_home_vol volume)"
    echo "  Memory Bank: /root/memory-bank (from $memory_bank_vol volume)"
    echo "  Plugins installed in container persist across YOUR sessions (user-isolated)"
}

#==============================================================================
# Daemon commands
#==============================================================================

daemon_status() {
    info "HAL-9000 Orchestrator Status"
    echo ""

    # Check if parent container exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        local status
        status=$(docker inspect --format '{{.State.Status}}' "$PARENT_CONTAINER" 2>/dev/null || echo "unknown")

        case "$status" in
            running)
                success "Parent container: Running"

                # Get uptime
                local started_at
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$PARENT_CONTAINER" 2>/dev/null || echo "")
                if [[ -n "$started_at" ]]; then
                    info "  Started: $started_at"
                fi

                # Check ChromaDB server
                if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                    success "  ChromaDB server: Running on port 8000"
                else
                    warn "  ChromaDB server: Not responding"
                fi

                # Count workers
                local worker_count
                worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')
                info "  Active workers: $worker_count"

                # List workers
                if [[ "$worker_count" -gt 0 ]]; then
                    echo ""
                    info "Workers:"
                    docker ps --filter "name=hal9000-worker" --format '  {{.Names}}: {{.Status}}' 2>/dev/null
                fi
                ;;
            exited)
                warn "Parent container: Stopped"
                local exit_code
                exit_code=$(docker inspect --format '{{.State.ExitCode}}' "$PARENT_CONTAINER" 2>/dev/null || echo "?")
                info "  Exit code: $exit_code"
                ;;
            *)
                warn "Parent container: $status"
                ;;
        esac
    else
        warn "Parent container: Not found"
        info "  Run 'hal-9000 daemon start' to create"
    fi

    # Check for shared volumes
    echo ""
    info "Shared volumes:"
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if docker volume inspect "$vol" >/dev/null 2>&1; then
            success "  $vol: exists"
        else
            info "  $vol: not created"
        fi
    done
}

daemon_start() {
    info "Starting HAL-9000 orchestrator..."

    # Check if already running
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        success "Orchestrator already running"
        daemon_status
        return 0
    fi

    # Remove stopped container if exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped parent container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1 || true
    fi

    # Pull image if not available
    if ! docker image inspect "$PARENT_IMAGE" >/dev/null 2>&1; then
        info "Pulling parent image: $PARENT_IMAGE"
        docker pull "$PARENT_IMAGE" || {
            warn "Could not pull image, trying local build..."
            local docker_dir="${HAL9000_HOME}/../hal-9000/plugins/hal-9000/docker"
            if [[ -f "$docker_dir/Dockerfile.parent" ]]; then
                docker build -f "$docker_dir/Dockerfile.parent" -t "$PARENT_IMAGE" "$docker_dir"
            else
                error "Parent image not available. Build with: make build-parent" 3
            fi
        }
    fi

    # Create shared volumes if they don't exist
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if ! docker volume inspect "$vol" >/dev/null 2>&1; then
            info "Creating volume: $vol"
            docker volume create "$vol" >/dev/null
        fi
    done

    # Ensure hal9000 directories exist
    mkdir -p "$HAL9000_HOME/sessions"
    mkdir -p "$HAL9000_HOME/logs"
    mkdir -p "$HAL9000_HOME/config"

    # Initialize seccomp profile on host filesystem
    # Required for worker containers - Docker reads profile before container starts
    init_seccomp_profile

    # Store API key to secret file if available (security: avoid env var exposure)
    store_api_key_secret

    # Start parent container
    info "Starting parent container..."
    local docker_args=(
        docker run -d
        --name "$PARENT_CONTAINER"
        --restart unless-stopped
        --memory "${HAL9000_PARENT_MEMORY:-4g}"
        --cpus "${HAL9000_PARENT_CPUS:-2}"
        -v /var/run/docker.sock:/var/run/docker.sock
        -v "${HAL9000_HOME}:/root/.hal9000"
        -v hal9000-chromadb:/data/chromadb
        -v hal9000-memorybank:/data/membank
        -v hal9000-plugins:/data/plugins
    )

    # SECURITY: Mount API key as secret file instead of environment variable
    # This prevents exposure via 'docker inspect' or /proc/1/environ
    local secret_path
    secret_path=$(get_api_key_secret_path)
    if [[ -n "$secret_path" ]]; then
        docker_args+=(-v "${secret_path}:/run/secrets/anthropic_key:ro")
    fi

    docker_args+=("$PARENT_IMAGE")

    if "${docker_args[@]}" >/dev/null; then
        # Wait for ChromaDB to start
        info "Waiting for ChromaDB server..."
        local max_wait=30
        local waited=0
        while [[ $waited -lt $max_wait ]]; do
            if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                success "ChromaDB server ready"
                break
            fi
            sleep 1
            ((waited++))
        done

        if [[ $waited -ge $max_wait ]]; then
            warn "ChromaDB server did not respond within ${max_wait}s"
        fi

        success "Orchestrator started"
        echo ""
        daemon_status
    else
        error "Failed to start parent container" 3
    fi
}

daemon_stop() {
    info "Stopping HAL-9000 orchestrator..."

    # Check for running workers
    local worker_count
    worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')

    if [[ "$worker_count" -gt 0 ]]; then
        warn "There are $worker_count active workers"
        info "Workers will lose their network connection to ChromaDB"
        echo ""
        read -p "Stop anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Cancelled"
            return 0
        fi
    fi

    # Stop parent container
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Stopping parent container..."
        docker stop "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Orchestrator stopped"
    elif docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Container removed"
    else
        info "Orchestrator not running"
    fi
}

daemon_restart() {
    info "Restarting HAL-9000 orchestrator..."
    daemon_stop
    sleep 2
    daemon_start
}

handle_daemon_command() {
    local cmd="${1:-}"

    case "$cmd" in
        start)
            daemon_start
            ;;
        stop)
            daemon_stop
            ;;
        status)
            daemon_status
            ;;
        restart)
            daemon_restart
            ;;
        ""|help)
            echo "Usage: hal-9000 daemon <command>"
            echo ""
            echo "Commands:"
            echo "  start    Start the orchestrator (parent container with ChromaDB server)"
            echo "  stop     Stop the orchestrator"
            echo "  status   Show orchestrator status and worker count"
            echo "  restart  Restart the orchestrator"
            ;;
        *)
            error "Unknown daemon command: $cmd" 2
            ;;
    esac
}

#==============================================================================
# Pool management
#==============================================================================

handle_pool_command() {
    local cmd="${1:-}"
    shift || true

    # Handle help without requiring parent
    if [[ "$cmd" == "help" ]] || [[ -z "$cmd" ]]; then
        echo "Usage: hal-9000 pool <command> [options]"
        echo ""
        echo "Manage the worker pool for fast container startup."
        echo ""
        echo "Commands:"
        echo "  start [--min-warm N] [--max-warm N]  Start the pool manager"
        echo "  stop                                  Stop the pool manager"
        echo "  status                                Show pool status"
        echo "  scale <n>                             Scale to n warm workers"
        echo "  cleanup                               Force cleanup idle workers"
        echo "  warm                                  Create a single warm worker"
        echo ""
        echo "Options:"
        echo "  --min-warm N      Minimum warm workers (default: 2)"
        echo "  --max-warm N      Maximum warm workers (default: 5)"
        echo "  --idle-timeout N  Seconds before cleanup (default: 300)"
        return 0
    fi

    # Check if parent is running for other commands
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        error "Parent container not running. Start with: hal-9000 daemon start" 3
    fi

    case "$cmd" in
        start)
            info "Starting pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh start "$@"
            ;;
        stop)
            info "Stopping pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh stop
            ;;
        status)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh status
            ;;
        scale)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh scale "$@"
            ;;
        cleanup)
            info "Cleaning up idle workers..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh cleanup
            ;;
        warm)
            info "Creating warm worker..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh warm
            ;;
        *)
            error "Unknown pool command: $cmd" 2
            ;;
    esac
}

#==============================================================================
# Via-parent launch (DinD mode)
#==============================================================================

launch_via_parent() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local detach="$6"
    local abs_project_path
    local image_base

    abs_project_path=$(cd "$project_dir" && pwd)
    image_base=$(get_container_image_base)

    # Check if parent is running
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        warn "Parent container not running"
        echo ""
        read -p "Start the orchestrator now? [Y/n] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Nn]$ ]]; then
            daemon_start
        else
            error "Cannot launch via parent without orchestrator. Run 'hal-9000 daemon start' first." 3
        fi
    fi

    info "Launching worker via parent container..."
    info "  Worker: $session_name"
    info "  Profile: $profile"
    info "  Project: $abs_project_path"

    # Build spawn-worker command
    local spawn_args=()
    spawn_args+=(-n "$session_name")

    if [[ "$detach" == "true" ]]; then
        spawn_args+=(-d)
    fi

    # Use worker image based on profile
    local worker_image="${image_base}:${profile}"
    spawn_args+=(-i "$worker_image")

    # Project directory - use HOST path because spawn-worker.sh
    # talks to the host's Docker daemon via socket
    spawn_args+=("$abs_project_path")

    # Store API key to secret file if provided (security: avoid env var exposure)
    if [[ -n "$api_key" ]]; then
        ANTHROPIC_API_KEY="$api_key" store_api_key_secret
    else
        store_api_key_secret
    fi

    # Execute spawn-worker.sh inside parent container
    # SECURITY: Do not pass API key as environment variable
    # The secret file is already mounted into the parent container via HAL9000_HOME
    local exec_args=(docker exec)

    if [[ "$detach" != "true" ]]; then
        exec_args+=(-it)
    fi

    # Parent container has access to secrets via /root/.hal9000/secrets mount
    exec_args+=("$PARENT_CONTAINER" /scripts/spawn-worker.sh "${spawn_args[@]}")

    info "Executing: ${exec_args[*]}"

    if "${exec_args[@]}"; then
        if [[ "$detach" == "true" ]]; then
            success "Worker spawned in background: $session_name"
            info "Attach with: docker exec -it $session_name bash"
        fi
    else
        error "Failed to spawn worker via parent" 3
    fi
}

#==============================================================================
# Profile detection (See: hal-9000-installation-setup.md)
#==============================================================================

detect_profile() {
    local target_dir="${1:-.}"

    # Check for Java project
    if [[ -f "$target_dir/pom.xml" ]] || [[ -f "$target_dir/build.gradle" ]] || [[ -f "$target_dir/build.gradle.kts" ]]; then
        echo "java"
        return 0
    fi

    # Check for Python project
    if [[ -f "$target_dir/pyproject.toml" ]] || [[ -f "$target_dir/Pipfile" ]] || [[ -f "$target_dir/requirements.txt" ]]; then
        echo "python"
        return 0
    fi

    # Check for Node.js project
    if [[ -f "$target_dir/package.json" ]]; then
        echo "node"
        return 0
    fi

    # Default to base
    echo "$DEFAULT_PROFILE"
}

#==============================================================================
# Session management
#==============================================================================

get_session_name() {
    local project_dir="${1:-.}"
    local abs_path
    abs_path=$(cd "$project_dir" && pwd)

    # Hash the path to create deterministic session name
    # Use base64 to make it readable but unique
    local hash
    hash=$(echo -n "$abs_path" | shasum -a 256 | cut -c1-8)

    local basename
    basename=$(basename "$abs_path")

    # Clean basename for tmux (alphanumeric, hyphen, underscore only)
    basename=$(echo "$basename" | sed 's/[^a-zA-Z0-9_-]/-/g')

    echo "hal-9000-${basename}-${hash}"
}

verify_prerequisites() {
    local missing_tools=()

    # Check for required tools
    for tool in docker bash; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        fi
    done

    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        error "Missing required tools: ${missing_tools[*]}" 3
    fi

    # Check Docker daemon
    if ! docker ps &> /dev/null; then
        error "Docker daemon not running or not accessible" 3
    fi

    # SECURITY: Check for and reject API key in environment variable
    check_api_key_security

    # Check authentication - secret file OR subscription login
    local secret_file="${HAL9000_SECRETS_DIR}/anthropic_key"
    if [[ -f "$secret_file" ]]; then
        success "Authentication: API key (secure file: $secret_file)"
    elif check_subscription_auth; then
        success "Authentication: Subscription login detected"
    else
        echo ""
        warn "No authentication configured"
        echo ""
        info "Option 1: Subscription login (recommended)"
        echo "  hal-9000 /login"
        echo ""
        info "Option 2: API key (via secure file)"
        echo "  echo 'sk-ant-api03-...' > ~/.hal9000/secrets/anthropic_key"
        echo "  chmod 400 ~/.hal9000/secrets/anthropic_key"
        echo "  Get key at: https://console.anthropic.com/settings/keys"
        echo ""
        error "Run 'hal-9000 /login' to authenticate with your Claude subscription" 4
    fi

    success "Prerequisites verified"
}

#==============================================================================
# Session initialization
#==============================================================================

init_session() {
    local project_dir="${1:-.}"
    local profile="${2:-$(detect_profile "$project_dir")}"
    local session_name="${3:-$(get_session_name "$project_dir")}"
    local shell_mode="${4:-false}"
    local abs_project_path
    local image_base

    abs_project_path=$(cd "$project_dir" && pwd)
    image_base=$(get_container_image_base)

    # Ensure shared Docker volumes exist
    ensure_volumes

    # Create session metadata directory
    local session_dir="$HAL9000_HOME/claude/$session_name"
    mkdir -p "$session_dir"

    # Create session metadata
    cat > "$session_dir/.hal-9000-session.json" << METADATA
{
  "name": "$session_name",
  "profile": "$profile",
  "project_dir": "$abs_project_path",
  "created_at": "$(date -Iseconds)",
  "shell_mode": $shell_mode,
  "container_image": "${image_base}:${profile}",
  "hal-9000_version": "$SCRIPT_VERSION"
}
METADATA

    echo "$session_name"
}

#==============================================================================
# Session management
#==============================================================================

# List running hal-9000 sessions
list_sessions() {
    info "Running hal-9000 sessions:"
    echo ""

    local sessions
    sessions=$(docker ps --filter "label=hal9000.session=true" --format "table {{.Names}}\t{{.Status}}\t{{.Label \"hal9000.project\"}}" 2>/dev/null)

    if [[ -z "$sessions" ]] || [[ "$sessions" == "NAMES"* && $(echo "$sessions" | wc -l) -eq 1 ]]; then
        info "  No running sessions"
        echo ""
        info "Start a new session with: hal-9000 /path/to/project"
    else
        echo "$sessions"
    fi
}

# Attach to an existing session
attach_session() {
    local session_name="${1:-}"

    if [[ -z "$session_name" ]]; then
        # If no session specified, list and pick
        local sessions
        sessions=$(docker ps --filter "label=hal9000.session=true" --format "{{.Names}}" 2>/dev/null)

        if [[ -z "$sessions" ]]; then
            error "No running sessions. Start one with: hal-9000 /path/to/project" 5
        fi

        local count
        count=$(echo "$sessions" | wc -l | tr -d ' ')

        if [[ "$count" -eq 1 ]]; then
            session_name="$sessions"
            info "Attaching to: $session_name"
        else
            info "Multiple sessions running. Specify one:"
            echo "$sessions" | while read -r s; do
                echo "  hal-9000 attach $s"
            done
            exit 1
        fi
    fi

    # Check if session exists
    if ! docker ps --format '{{.Names}}' | grep -q "^${session_name}$"; then
        error "Session not found: $session_name" 5
    fi

    # Get the shell_mode flag from the container labels
    local shell_mode
    shell_mode=$(docker inspect --format='{{index .Config.Labels "hal9000.shell_mode"}}' "$session_name" 2>/dev/null || echo "false")

    info "Attaching to session: $session_name"
    echo ""

    if [[ "$shell_mode" == "true" ]]; then
        # Shell mode: just attach to bash
        # SECURITY: For shell mode, we export ANTHROPIC_API_KEY_FILE path only
        # User can read from file if needed, but key is not in environment
        info "Type 'exit' to disconnect"
        docker exec -it "$session_name" bash -c '
            if [ -f /run/secrets/anthropic_key ]; then
              export ANTHROPIC_API_KEY_FILE=/run/secrets/anthropic_key
            fi
            exec bash
        '
    else
        # Normal mode: restore session state and run Claude with proper TTY
        info "Launching Claude..."
        docker exec -it "$session_name" bash -c '
            # Restore Claude session state from persistent volume
            if [ -f /root/.claude-session/.claude.json ]; then
              cp /root/.claude-session/.claude.json /root/.claude.json
            fi
            # SECURITY: Read API key only at command execution time
            # Using VAR=value command syntax keeps the key in claude process only
            if [ -f /run/secrets/anthropic_key ]; then
              ANTHROPIC_API_KEY=$(cat /run/secrets/anthropic_key) claude
            else
              claude
            fi
            # Save Claude session state back to persistent volume after exit
            [ -f /root/.claude.json ] && cp /root/.claude.json /root/.claude-session/.claude.json
        '
    fi
}

# Kill a session
kill_session() {
    local session_name="${1:-}"

    if [[ -z "$session_name" ]]; then
        error "Usage: hal-9000 kill <session-name>" 2
    fi

    if ! docker ps --format '{{.Names}}' | grep -q "^${session_name}$"; then
        error "Session not found: $session_name" 5
    fi

    info "Stopping session: $session_name"
    docker stop "$session_name" >/dev/null 2>&1
    docker rm "$session_name" >/dev/null 2>&1
    success "Session stopped: $session_name"
}

#==============================================================================
# Container launching
#==============================================================================

launch_container_session() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local detach="${6:-false}"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)

    # Store API key to secret file if provided (security: avoid env var exposure)
    if [[ -n "$api_key" ]]; then
        ANTHROPIC_API_KEY="$api_key" store_api_key_secret
    else
        store_api_key_secret
    fi

    # Determine container image: local profile > remote profile > base
    local image_base=$(get_container_image_base)
    local container_image="${image_base}:${profile}"

    # Check for local profile and build if needed
    if local_image=$(get_local_profile_image "$profile" 2>/dev/null); then
        # Local profile exists - build it if not already built
        build_local_profile "$profile"
        container_image="$local_image"
        info "Using local profile: $profile"
    fi

    # Check if session already exists
    local session_exists=false
    if docker ps --format '{{.Names}}' | grep -q "^${session_name}$"; then
        session_exists=true
        info "Session already running: $session_name"
        if [[ "$detach" == "true" ]]; then
            info "Use 'hal-9000 attach $session_name' to attach"
            return 0
        else
            # Reuse existing session (will continue to cleanup code below if interactive)
            shell_mode=$(docker inspect --format='{{index .Config.Labels "hal9000.shell_mode"}}' "$session_name" 2>/dev/null || echo "false")
        fi
    fi

    # Verify image exists or fall back to base
    if ! docker inspect "$container_image" &> /dev/null; then
        warn "Image not found: $container_image, using base profile"
        container_image="${image_base}:base"
    fi

    info "Launching Claude in container..."
    info "  Profile: $profile"
    info "  Session: $session_name"
    info "  Directory: $abs_project_path"

    # SECURITY: Use user-scoped volumes for isolation
    local claude_home_vol
    local claude_session_vol
    local memory_bank_vol
    local user_hash
    claude_home_vol=$(get_claude_home_volume)
    claude_session_vol=$(get_claude_session_volume)
    memory_bank_vol=$(get_memory_bank_volume)
    user_hash=$(get_user_hash)

    info "  User isolation: hash=${user_hash}"

    # Build docker run command
    # NO --rm: container persists for reattachment
    # Uses user-scoped Docker volumes for CLAUDE_HOME and memory bank
    local docker_args=(
        docker run
        -d
        --name "$session_name"
        --label "hal9000.session=true"
        --label "hal9000.project=$abs_project_path"
        --label "hal9000.profile=$profile"
        --label "hal9000.shell_mode=$shell_mode"
        --label "hal9000.user_hash=$user_hash"
        -v "$abs_project_path:/workspace"
        -v "${claude_home_vol}:/root/.claude"
        -v "${claude_session_vol}:/root/.claude-session"
        -v "${memory_bank_vol}:/root/memory-bank"
        -v /var/run/docker.sock:/var/run/docker.sock
        -e "CLAUDE_HOME=/root/.claude"
        -e "HAL9000_SESSION=$session_name"
        -e "HAL9000_PROJECT_DIR=$abs_project_path"
        -e "MEMORY_BANK_ROOT=/root/memory-bank"
        -e "HAL9000_USER_HASH=$user_hash"
    )

    # SECURITY: Mount API key as secret file instead of environment variable
    # This prevents exposure via 'docker inspect' or /proc/1/environ
    local secret_path
    secret_path=$(get_api_key_secret_path)
    if [[ -n "$secret_path" ]]; then
        docker_args+=(-v "${secret_path}:/run/secrets/anthropic_key:ro")
        info "  Auth: API key (mounted as secret file)"
    else
        info "  Auth: Subscription (from volume) or interactive login"
    fi

    # Add image and startup command
    docker_args+=(-w "/workspace" "$container_image")

    # Start container with Claude or shell
    # Container keeps running with bash so user can access shell afterward
    # Key: Restore .claude.json from persistent session volume before running Claude
    # Container startup: just keep it alive indefinitely
    # Claude will be launched via docker exec, which provides proper TTY
    docker_args+=(bash -c "
        # Keep container alive for docker exec to attach
        # User will interact via 'docker exec -it' which provides the TTY
        tail -f /dev/null
    ")

    # Start the container (unless it already exists)
    if [[ "$session_exists" != "true" ]]; then
        info "Starting container..."
        if ! "${docker_args[@]}" >/dev/null; then
            error "Failed to start container" 3
        fi
    fi

    if [[ "$detach" == "true" ]]; then
        # Detached mode: don't attach, just report
        if [[ "$session_exists" == "true" ]]; then
            success "Session already running in background: $session_name"
        else
            success "Session started in background: $session_name"
        fi
        echo ""
        info "Attach with: hal-9000 attach $session_name"
        info "Or simply: hal-9000 attach (if only one session)"
    else
        # Interactive mode: attach and clean up when user exits
        if [[ "$session_exists" == "true" ]]; then
            info "Reattaching to session: $session_name"
        else
            info "Attaching to session: $session_name"
        fi
        echo ""

        if [[ "$shell_mode" == "true" ]]; then
            # Shell mode: just attach to bash
            # SECURITY: For shell mode, we export ANTHROPIC_API_KEY_FILE path only
            # User can read from file if needed, but key is not in environment
            info "Type 'exit' to disconnect"
            docker exec -it "$session_name" bash -c '
                if [ -f /run/secrets/anthropic_key ]; then
                  export ANTHROPIC_API_KEY_FILE=/run/secrets/anthropic_key
                fi
                exec bash
            '
        else
            # Normal mode: restore session state and run Claude with proper TTY
            info "Launching Claude..."
            docker exec -it "$session_name" bash -c '
                # FILE-BASED SESSION PERSISTENCE:
                # State is persisted across session attachments via .claude.json file
                # Restore Claude session state from persistent volume
                if [ -f /root/.claude-session/.claude.json ]; then
                  cp /root/.claude-session/.claude.json /root/.claude.json
                fi
                # SECURITY: Read API key only at command execution time
                # Using VAR=value command syntax keeps the key in claude process only
                if [ -f /run/secrets/anthropic_key ]; then
                  ANTHROPIC_API_KEY=$(cat /run/secrets/anthropic_key) claude
                else
                  claude
                fi
                # Save Claude session state back to persistent volume after exit
                # This enables resuming the exact session on next attach
                [ -f /root/.claude.json ] && cp /root/.claude.json /root/.claude-session/.claude.json
            '
        fi

        # Clean up container after interactive session ends
        info "Cleaning up container..."
        docker stop "$session_name" >/dev/null 2>&1
        docker rm "$session_name" >/dev/null 2>&1
        success "Session ended: $session_name"
    fi
}

#==============================================================================
# Main entry point
#==============================================================================

main() {
    local project_dir="."
    local profile=""
    local session_name=""
    local api_key=""
    local shell_mode=false
    local detach=false
    local verify_only=false
    local via_parent=false

    # Ensure volumes exist and are initialized with pristine config
    # This runs once on first use, subsequent launches reuse initialized volumes
    ensure_volumes

    # Handle daemon subcommand first
    if [[ "${1:-}" == "daemon" ]]; then
        shift
        handle_daemon_command "$@"
        exit 0
    fi

    # Handle pool subcommand
    if [[ "${1:-}" == "pool" ]]; then
        shift
        handle_pool_command "$@"
        exit 0
    fi

    # Handle session management subcommands
    if [[ "${1:-}" == "sessions" ]]; then
        list_sessions
        exit 0
    fi

    if [[ "${1:-}" == "attach" ]]; then
        shift
        attach_session "$@"
        exit 0
    fi

    if [[ "${1:-}" == "kill" ]]; then
        shift
        kill_session "$@"
        exit 0
    fi

    if [[ "${1:-}" == "profiles" ]]; then
        shift
        case "${1:-}" in
            build)
                if [[ -z "${2:-}" ]]; then
                    error "Usage: hal-9000 profiles build <profile-name>" 2
                fi
                build_local_profile "$2"
                exit 0
                ;;
            list|"")
                list_local_profiles
                exit 0
                ;;
            *)
                error "Unknown profiles command: $1" 2
                ;;
        esac
    fi

    # Handle claude subcommand passthrough (plugin, mcp, doctor, etc.)
    if is_claude_subcommand "${1:-}"; then
        run_claude_command "$@"
        exit $?
    fi

    # Parse arguments for interactive session mode
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --profile|-p)
                profile="$2"
                shift 2
                ;;
            --shell|-s)
                shell_mode=true
                shift
                ;;
            --name)
                session_name="$2"
                shift 2
                ;;
            --api-key)
                error "SECURITY: --api-key is deprecated and no longer supported.

API keys passed via command line or environment are a security risk:
  - Visible in process listings (ps aux)
  - Visible via 'docker inspect'
  - May be logged in shell history

Use file-based secrets instead:
  echo 'your-api-key' > ~/.hal9000/secrets/anthropic_key
  chmod 400 ~/.hal9000/secrets/anthropic_key" 1
                ;;
            --detach|-d)
                detach=true
                shift
                ;;
            --via-parent)
                via_parent=true
                shift
                ;;
            --verify)
                verify_only=true
                shift
                ;;
            --setup)
                setup_auth
                exit 0
                ;;
            --diagnose)
                show_diagnostics
                exit 0
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            --version|-v)
                show_version
                exit 0
                ;;
            -*)
                error "Unknown option: $1" 2
                ;;
            *)
                project_dir="$1"
                shift
                ;;
        esac
    done

    # Verify prerequisites (skip API key check for via-parent mode)
    if [[ "$via_parent" == "true" ]]; then
        # For via-parent, just check Docker is available
        if ! command -v docker &> /dev/null; then
            error "Docker not found" 3
        fi
        if ! docker ps &> /dev/null; then
            error "Docker daemon not running" 3
        fi
    else
        if ! verify_prerequisites; then
            exit 1
        fi
    fi

    # Validate profile if explicitly provided
    if [[ -n "$profile" ]]; then
        case "$profile" in
            base|python|node|java)
                # Valid profile
                ;;
            *)
                error "Invalid profile: $profile (must be: base, python, node, java)" 2
                ;;
        esac
    fi

    if [[ "$verify_only" == "true" ]]; then
        exit 0
    fi

    # Change to project directory
    if [[ ! -d "$project_dir" ]]; then
        error "Directory not found: $project_dir" 1
    fi

    # Auto-detect profile if not specified
    if [[ -z "$profile" ]]; then
        profile=$(detect_profile "$project_dir")
        info "Auto-detected profile: $profile"
    fi

    # Initialize session
    if [[ -z "$session_name" ]]; then
        session_name=$(get_session_name "$project_dir")
    fi

    # Launch via parent or direct
    if [[ "$via_parent" == "true" ]]; then
        # DinD mode: spawn worker via parent container
        info "Using via-parent mode (DinD)"
        launch_via_parent "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" "$detach" || error "Failed to launch via parent" 3
    else
        # Direct mode: launch container directly
        info "Initializing session..."
        init_session "$project_dir" "$profile" "$session_name" "$shell_mode" || error "Failed to initialize session" 1

        success "Session ready: $session_name"

        # Launch container
        launch_container_session "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" "$detach" || error "Failed to launch container" 3
    fi
}

# Run main if script is executed directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
