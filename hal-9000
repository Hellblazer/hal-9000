#!/bin/bash
# hal-9000 - Containerized Claude CLI launcher
# Opens Claude inside a tmux session within a hal-9000 container

set -Eeuo pipefail

readonly SCRIPT_NAME="hal-9000"
readonly SCRIPT_VERSION="2.0.0"
readonly HAL9000_HOME="${HOME}/.hal9000"
readonly HAL9000_PROFILES_DIR="${HAL9000_HOME}/profiles"
readonly HAL9000_CONFIG_FILE="${HAL9000_HOME}/config"
readonly PARENT_CONTAINER="hal9000-parent"
readonly PARENT_IMAGE="ghcr.io/hellblazer/hal-9000:parent"
readonly DEFAULT_PROFILE="base"
readonly DEFAULT_CONTAINER_IMAGE_BASE="ghcr.io/hellblazer/hal-9000"

# Shared Docker volumes for CLAUDE_HOME and session state - all workers share these
readonly CLAUDE_HOME_VOLUME="hal9000-claude-home"
readonly CLAUDE_SESSION_VOLUME="hal9000-claude-session"
readonly MEMORY_BANK_VOLUME="hal9000-memory-bank"

# Claude subcommands that should be passed through to container
readonly CLAUDE_SUBCOMMANDS="mcp|plugin|doctor|install|setup-token|update"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

#==============================================================================
# Error handling
#==============================================================================

error() {
    local exit_code="${2:-1}"
    echo -e "${RED}✗ Error: $1${NC}" >&2
    exit "$exit_code"
}

warn() {
    echo -e "${YELLOW}⚠ Warning: $*${NC}" >&2
}

info() {
    echo -e "${BLUE}ℹ $*${NC}"
}

success() {
    echo -e "${GREEN}✓ $*${NC}"
}

#==============================================================================
# Configuration and local profiles
#==============================================================================

# Validate profile name (prevent path traversal attacks)
validate_profile_name() {
    local name="$1"

    # Only allow alphanumeric, dash, underscore
    if [[ ! "$name" =~ ^[a-zA-Z0-9_-]+$ ]]; then
        return 1
    fi

    # Additional check: reject common path traversal patterns
    if [[ "$name" == ".."* ]] || [[ "$name" == *".."* ]]; then
        return 1
    fi

    return 0
}

# Load configuration from ~/.hal9000/config (safe parsing, no arbitrary code execution)
load_config() {
    if [[ ! -f "$HAL9000_CONFIG_FILE" ]]; then
        return 0
    fi

    # Safely parse config file - only allow VAR=value assignments with known variables
    while IFS='=' read -r key value; do
        # Skip empty lines and comments
        [[ -z "$key" ]] && continue
        [[ "$key" =~ ^[[:space:]]*# ]] && continue

        # Trim whitespace
        key=$(echo "$key" | xargs)
        value=$(echo "$value" | xargs)

        # Only allow specific known variables (whitelist approach)
        case "$key" in
            CONTAINER_IMAGE_BASE)
                export CONTAINER_IMAGE_BASE="$value"
                ;;
            *)
                # Silently ignore unknown variables - do not execute arbitrary code
                :
                ;;
        esac
    done < "$HAL9000_CONFIG_FILE"
}

# Get container image base with precedence: env var > config file > default
get_container_image_base() {
    # Environment variable takes highest precedence
    if [[ -n "${HAL9000_CONTAINER_IMAGE_BASE:-}" ]]; then
        echo "$HAL9000_CONTAINER_IMAGE_BASE"
        return 0
    fi

    # Load and check config file
    load_config
    if [[ -n "${CONTAINER_IMAGE_BASE:-}" ]]; then
        echo "$CONTAINER_IMAGE_BASE"
        return 0
    fi

    # Default
    echo "$DEFAULT_CONTAINER_IMAGE_BASE"
}

# Check if a local profile exists and optionally build it
get_local_profile_image() {
    local profile="$1"

    # Validate profile name (prevent path traversal)
    if ! validate_profile_name "$profile"; then
        error "Invalid profile name: $profile (only alphanumeric, dash, underscore allowed)" 2
    fi

    local local_profile_dir="${HAL9000_PROFILES_DIR}/${profile}"

    if [[ -f "${local_profile_dir}/Dockerfile" ]]; then
        # Local profile exists - return the image name
        echo "hal-9000-local-${profile}:latest"
        return 0
    fi

    # No local profile
    return 1
}

# Build a local profile if it exists and is not already built
build_local_profile() {
    local profile="$1"

    # Validate profile name (prevent path traversal)
    if ! validate_profile_name "$profile"; then
        error "Invalid profile name: $profile (only alphanumeric, dash, underscore allowed)" 2
    fi

    local local_profile_dir="${HAL9000_PROFILES_DIR}/${profile}"

    if [[ ! -f "${local_profile_dir}/Dockerfile" ]]; then
        # No local profile to build
        return 0
    fi

    local image_name="hal-9000-local-${profile}:latest"

    # Check if already built
    if docker inspect "$image_name" &>/dev/null; then
        return 0
    fi

    # Build the local profile
    info "Building local profile: $profile"
    if ! docker build -f "${local_profile_dir}/Dockerfile" -t "$image_name" "$local_profile_dir" >/dev/null 2>&1; then
        error "Failed to build local profile: $profile" 3
    fi
    success "Local profile built: $profile"
}

# List available local profiles
list_local_profiles() {
    if [[ ! -d "$HAL9000_PROFILES_DIR" ]]; then
        return 0
    fi

    echo ""
    echo "Local profiles available in $HAL9000_PROFILES_DIR:"
    for profile_dir in "$HAL9000_PROFILES_DIR"/*; do
        if [[ -d "$profile_dir" && -f "${profile_dir}/Dockerfile" ]]; then
            local profile_name=$(basename "$profile_dir")
            local image_name="hal-9000-local-${profile_name}:latest"

            if docker inspect "$image_name" &>/dev/null; then
                echo "  ✓ $profile_name (built)"
            else
                echo "  ○ $profile_name (not yet built)"
            fi
        fi
    done
    echo ""
}

#==============================================================================
# Claude command passthrough
#==============================================================================

# Ensure shared volumes exist
ensure_volumes() {
    docker volume create "$CLAUDE_HOME_VOLUME" >/dev/null 2>&1 || true
    docker volume create "$CLAUDE_SESSION_VOLUME" >/dev/null 2>&1 || true
    docker volume create "$MEMORY_BANK_VOLUME" >/dev/null 2>&1 || true

    # Initialize volumes with pristine Claude config on first creation
    # This ensures containers start in a clean state with no host-specific paths
    docker run --rm \
        -v "${CLAUDE_SESSION_VOLUME}:/session" \
        -v "${CLAUDE_HOME_VOLUME}:/claude-home" \
        alpine:latest \
        sh -c '
            # Initialize session volume with pristine Claude config
            if [ ! -f /session/.initialized ]; then
                cat > /session/.claude.json << '"'"'EOF'"'"'
{
  "theme": "auto",
  "installMethod": "native",
  "autoUpdates": false
}
EOF
                touch /session/.initialized
            fi

            # Initialize CLAUDE_HOME structure if empty
            if [ ! -d /claude-home/plugins ]; then
                mkdir -p /claude-home/plugins
                touch /claude-home/.initialized
            fi
        ' >/dev/null 2>&1 || true
}

# Check if subscription login is configured in the shared volume
# Returns 0 if logged in, 1 if not
check_subscription_auth() {
    ensure_volumes

    # Check for auth credentials in the volume
    # Claude stores session data in .claude/
    docker run --rm \
        -v "${CLAUDE_HOME_VOLUME}:/root/.claude:ro" \
        alpine:latest \
        sh -c 'test -f /root/.claude/.credentials.json || test -f /root/.claude/statsig_user_id' \
        2>/dev/null
}

# Run a claude command inside a container with shared volumes
# Usage: run_claude_command <args...>
run_claude_command() {
    ensure_volumes

    local image_base
    image_base=$(get_container_image_base)
    local image="${image_base}:base"

    # Build docker args
    local docker_args=(
        docker run --rm -it
        -v "${CLAUDE_HOME_VOLUME}:/root/.claude"
        -v "${CLAUDE_SESSION_VOLUME}:/root/.claude-session"
        -v "${MEMORY_BANK_VOLUME}:/root/memory-bank"
        -e "CLAUDE_HOME=/root/.claude"
        -e "MEMORY_BANK_ROOT=/root/memory-bank"
    )

    # Pass through API key if available
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
    fi

    # Run claude with session state restoration and persistence
    # This ensures commands like /login save credentials properly
    "${docker_args[@]}" "$image" bash -c '
        # Restore session state from persistent volume
        if [ -f /root/.claude-session/.claude.json ]; then
          cp /root/.claude-session/.claude.json /root/.claude.json
        fi
        # Run the command
        claude "$@"
        # Save session state back to persistent volume
        [ -f /root/.claude.json ] && cp /root/.claude.json /root/.claude-session/.claude.json
    ' bash "$@"
}

# Check if arg is a claude subcommand that should be passed through
is_claude_subcommand() {
    local arg="${1:-}"
    # Match known subcommands (mcp, plugin, doctor, etc.)
    [[ "$arg" =~ ^($CLAUDE_SUBCOMMANDS)$ ]] && return 0
    # Match Claude slash commands like /login, /help, /status
    [[ "$arg" =~ ^/ ]] && [[ ! -d "$arg" ]] && return 0
    return 1
}

#==============================================================================
# Help and diagnostics
#==============================================================================

show_help() {
    cat << 'EOF'
hal-9000 - Containerized Claude CLI

USAGE:
  hal-9000 [OPTIONS] [DIRECTORY]           # Interactive Claude session
  hal-9000 <claude-command> [args...]      # Run claude command in container
  hal-9000 daemon <command>                # Manage orchestrator
  hal-9000 pool <command>                  # Manage worker pool
  hal-9000 sessions                        # List running sessions
  hal-9000 attach [session]                # Attach to existing session
  hal-9000 kill <session>                  # Stop a session

CLAUDE COMMANDS (passthrough to container):
  hal-9000 plugin list                     # List installed plugins
  hal-9000 plugin install <name>           # Install a plugin
  hal-9000 plugin marketplace add <url>    # Add a marketplace
  hal-9000 mcp list                        # List MCP servers
  hal-9000 mcp add <server>                # Add MCP server
  hal-9000 doctor                          # Check Claude health

  All 'claude' subcommands work - they run inside a container with the
  shared CLAUDE_HOME volume, so changes persist across all sessions.

DAEMON COMMANDS:
  hal-9000 daemon start    Start the orchestrator (parent container with ChromaDB)
  hal-9000 daemon stop     Stop the orchestrator
  hal-9000 daemon status   Show orchestrator status and worker count
  hal-9000 daemon restart  Restart the orchestrator

POOL COMMANDS:
  hal-9000 pool start      Start the worker pool manager
  hal-9000 pool stop       Stop the worker pool manager
  hal-9000 pool status     Show pool status (warm/busy workers)
  hal-9000 pool scale <n>  Scale to n warm workers
  hal-9000 pool cleanup    Force cleanup of idle workers

SESSION COMMANDS:
  hal-9000 sessions        List running hal-9000 sessions
  hal-9000 attach [name]   Reattach to an existing session (auto-selects if only one)
  hal-9000 kill <name>     Stop and remove a session

  Sessions persist after exiting. To reattach to a running container,
  use 'hal-9000 attach' and the container will drop you into a bash shell.

OPTIONS:
  --setup                Interactive setup for API key (first-time setup)
  --profile PROFILE      Profile: base, python, node, java (default: auto-detect)
  --shell, -s            Start bash shell instead of Claude
  --name NAME            Custom session name (default: auto from directory)
  --api-key KEY          Anthropic API key (or set ANTHROPIC_API_KEY env var)
  --detach, -d           Don't attach to tmux session after launch
  --via-parent           Launch worker via parent container (requires daemon running)
  --verify               Verify prerequisites and exit
  --diagnose             Show diagnostic information
  --help, -h             Show this help message
  --version, -v          Show version

ARGUMENTS:
  DIRECTORY              Project directory (default: current directory)

AUTHENTICATION:
  Option 1 - Subscription Login (recommended):
    hal-9000 /login      # Login once, persists for all sessions

  Option 2 - API Key:
    export ANTHROPIC_API_KEY=sk-ant-api03-...
    Get key at: https://console.anthropic.com/settings/keys
    Add to ~/.bashrc or ~/.zshrc to persist.

  Auth is stored in shared Docker volume - login once, use everywhere.

EXAMPLES:
  hal-9000                                    # Current dir, auto-detect profile
  hal-9000 --profile python                   # Force Python profile
  hal-9000 ~/projects/myapp                   # Specific directory
  hal-9000 --shell                            # Start with shell instead of Claude
  hal-9000 plugin install memory-bank         # Install plugin (persists)
  hal-9000 mcp list                           # List MCP servers
  hal-9000 --diagnose                         # Check setup

SESSION EXAMPLES:
  hal-9000 ~/project                          # Start new session, launches Claude
  # Inside Claude: type 'exit' to return to bash shell
  hal-9000 sessions                           # List all running sessions
  hal-9000 attach                             # Reattach to a session (auto-pick if one)
  hal-9000 attach hal-9000-myapp-abc123       # Reattach to specific session
  hal-9000 kill hal-9000-myapp-abc123         # Stop a session

DAEMON EXAMPLES:
  hal-9000 daemon start                       # Start the orchestrator
  hal-9000 daemon status                      # Check status and workers
  hal-9000 daemon stop                        # Stop the orchestrator
  hal-9000 --via-parent ~/project             # Launch worker via parent (DinD mode)

PROFILE COMMANDS:
  hal-9000 profiles                           # List available local profiles
  hal-9000 profiles list                      # List available local profiles (same as above)
  hal-9000 profiles build <name>              # Build a specific local profile

CUSTOM PROFILES:
  Create profiles in ~/.hal9000/profiles/:
    mkdir -p ~/.hal9000/profiles/ruby
    cat > ~/.hal9000/profiles/ruby/Dockerfile << 'EOF'
    FROM ghcr.io/hellblazer/hal-9000:base
    RUN gem install rails bundler
    EOF

  Then hal-9000 automatically detects and uses it:
    cd /path/to/ruby/project
    hal-9000

  See README-CUSTOM_PROFILES.md for complete guide.

ENVIRONMENT:
  ANTHROPIC_API_KEY              API key for Claude authentication
  HAL9000_CONTAINER_IMAGE_BASE   Override Docker image registry/base
                                 (e.g., "my-registry/my-hal-9000")
  HAL9000_HOME                   Override session storage (default: ~/.hal9000)
  DOCKER_SOCKET                  Docker socket path (default: /var/run/docker.sock)

CONFIGURATION:
  Config file: ~/.hal9000/config
  Set persistent defaults (e.g., custom image registry):
    cat > ~/.hal9000/config << 'EOF'
    CONTAINER_IMAGE_BASE=my-registry/hal-9000
    EOF

  Precedence: environment variable > config file > default

VOLUMES:
  hal9000-claude-home       Shared CLAUDE_HOME - plugins, credentials, config persist here
  hal9000-claude-session    Shared session state - .claude.json (auth state) persists here
  hal9000-memory-bank       Shared memory bank for cross-session context

For more information, see documentation at:
  https://github.com/hellblazer/hal-9000/blob/main/README.md
EOF
}

show_version() {
    echo "hal-9000 version $SCRIPT_VERSION"
    echo "hal-9000 plugin version $SCRIPT_VERSION"
}

setup_auth() {
    echo ""
    info "hal-9000 Authentication Setup"
    echo ""

    # Check if already configured
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        success "ANTHROPIC_API_KEY is already set"
        echo "  Current value: ${ANTHROPIC_API_KEY:0:15}..."
        echo ""
        read -p "Replace with a new key? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Keeping existing configuration"
            exit 0
        fi
    fi

    echo "Enter your Anthropic API key (or 'q' to quit):"
    echo "  Get one at: https://console.anthropic.com/settings/keys"
    echo ""
    read -p "API Key: " api_key_input

    if [[ "$api_key_input" == "q" ]] || [[ -z "$api_key_input" ]]; then
        info "Setup cancelled"
        exit 0
    fi

    # Validate format
    if [[ ! "$api_key_input" =~ ^sk-ant- ]]; then
        warn "Key doesn't look like an Anthropic API key (expected sk-ant-...)"
        read -p "Continue anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi

    # Detect shell config file
    local shell_rc=""
    if [[ -n "${ZSH_VERSION:-}" ]] || [[ "$SHELL" == *"zsh"* ]]; then
        shell_rc="$HOME/.zshrc"
    else
        shell_rc="$HOME/.bashrc"
    fi

    echo ""
    info "Where to save the API key?"
    echo "  1) $shell_rc (recommended - persists across sessions)"
    echo "  2) Current session only (export now, won't persist)"
    echo "  3) Cancel"
    echo ""
    read -p "Choice [1]: " -n 1 -r choice
    echo ""

    case "${choice:-1}" in
        1)
            # Check if already in file
            if grep -q "ANTHROPIC_API_KEY" "$shell_rc" 2>/dev/null; then
                warn "ANTHROPIC_API_KEY already exists in $shell_rc"
                read -p "Replace it? [y/N] " -n 1 -r
                echo ""
                if [[ $REPLY =~ ^[Yy]$ ]]; then
                    # Remove old line and add new
                    sed -i.bak '/export ANTHROPIC_API_KEY/d' "$shell_rc"
                else
                    info "Setup cancelled"
                    exit 0
                fi
            fi

            echo "" >> "$shell_rc"
            echo "# Anthropic API key for Claude Code (added by hal-9000)" >> "$shell_rc"
            echo "export ANTHROPIC_API_KEY=\"$api_key_input\"" >> "$shell_rc"

            success "Added to $shell_rc"
            echo ""
            info "To use now, run:"
            echo "  source $shell_rc"
            echo ""
            info "Or start a new terminal session."
            ;;
        2)
            export ANTHROPIC_API_KEY="$api_key_input"
            success "API key set for current session"
            echo ""
            info "Note: This won't persist. Run 'hal-9000 --setup' again to save permanently."
            ;;
        *)
            info "Setup cancelled"
            exit 0
            ;;
    esac

    echo ""
    success "Setup complete! You can now run: hal-9000"
}

show_diagnostics() {
    info "hal-9000 Diagnostics"
    echo ""

    # Check Docker
    if command -v docker &> /dev/null; then
        DOCKER_VERSION=$(docker --version)
        success "Docker: $DOCKER_VERSION"
    else
        error "Docker not found. Install Docker to use hal-9000." 3
    fi

    # Check tmux
    if command -v tmux &> /dev/null; then
        TMUX_VERSION=$(tmux -V)
        success "tmux: $TMUX_VERSION"
    else
        warn "tmux not found. Install tmux for full functionality."
    fi

    # Check bash version
    BASH_VERSION="${BASH_VERSION%.*}"
    if [[ "${BASH_VERSION}" =~ ^[5-9]|[0-9]{2,} ]]; then
        success "Bash: $BASH_VERSION (OK)"
    else
        warn "Bash version: $BASH_VERSION (recommend 5.0+)"
    fi

    # Check authentication
    echo ""
    info "Authentication:"
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        local key_preview="${ANTHROPIC_API_KEY:0:10}..."
        success "  API Key: Set ($key_preview)"
    else
        info "  API Key: Not set"
    fi

    if check_subscription_auth; then
        success "  Subscription: Logged in (credentials in volume)"
    else
        info "  Subscription: Not logged in"
        info "    → Run 'hal-9000 /login' to authenticate"
    fi

    # Check Docker volumes
    echo ""
    info "Docker Volumes:"
    if docker volume inspect "$CLAUDE_HOME_VOLUME" &>/dev/null; then
        success "  $CLAUDE_HOME_VOLUME (exists)"
        # Show volume size if possible
        local vol_path
        vol_path=$(docker volume inspect "$CLAUDE_HOME_VOLUME" --format '{{.Mountpoint}}' 2>/dev/null || true)
        if [[ -n "$vol_path" ]]; then
            info "    → Mountpoint: $vol_path"
        fi
    else
        info "  $CLAUDE_HOME_VOLUME (not yet created)"
    fi

    if docker volume inspect "$MEMORY_BANK_VOLUME" &>/dev/null; then
        success "  $MEMORY_BANK_VOLUME (exists)"
    else
        info "  $MEMORY_BANK_VOLUME (not yet created)"
    fi

    # Check hal9000 storage
    echo ""
    if [[ -d "$HAL9000_HOME" ]]; then
        SESSION_COUNT=$(find "$HAL9000_HOME/claude" -maxdepth 1 -type d 2>/dev/null | wc -l)
        success "hal9000 home: $HAL9000_HOME"
        info "  → Active sessions: $((SESSION_COUNT - 1))"
    else
        info "hal9000 home: $HAL9000_HOME (not yet created)"
    fi

    # Check Docker socket
    if [[ -S /var/run/docker.sock ]]; then
        success "Docker socket: /var/run/docker.sock"
    elif [[ -S "$HOME/.docker/run/docker.sock" ]]; then
        success "Docker socket: $HOME/.docker/run/docker.sock"
    else
        warn "Docker socket not found"
    fi

    # Check for container image
    echo ""
    local image_base
    image_base=$(get_container_image_base)
    if docker inspect "${image_base}:base" &> /dev/null; then
        success "Container image: ${image_base}:base (available)"
    else
        warn "Container image: ${image_base}:base (not found - run: make build-base)"
    fi

    echo ""
    info "System: $(uname -s) $(uname -m)"
    info "Current directory: $(pwd)"

    # Auth guidance
    echo ""
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        info "Authentication options:"
        echo "  1. Set ANTHROPIC_API_KEY environment variable"
        echo "  2. Run 'hal-9000 --shell' then 'claude /login' inside container"
        echo "  3. Run 'claude setup-token' on host for long-lived token"
    fi

    # Container config info
    echo ""
    info "Container config:"
    echo "  CLAUDE_HOME: /root/.claude (from $CLAUDE_HOME_VOLUME volume)"
    echo "  Memory Bank: /root/memory-bank (from $MEMORY_BANK_VOLUME volume)"
    echo "  Plugins installed in container persist across all sessions"
}

#==============================================================================
# Daemon commands
#==============================================================================

daemon_status() {
    info "HAL-9000 Orchestrator Status"
    echo ""

    # Check if parent container exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        local status
        status=$(docker inspect --format '{{.State.Status}}' "$PARENT_CONTAINER" 2>/dev/null || echo "unknown")

        case "$status" in
            running)
                success "Parent container: Running"

                # Get uptime
                local started_at
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$PARENT_CONTAINER" 2>/dev/null || echo "")
                if [[ -n "$started_at" ]]; then
                    info "  Started: $started_at"
                fi

                # Check ChromaDB server
                if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                    success "  ChromaDB server: Running on port 8000"
                else
                    warn "  ChromaDB server: Not responding"
                fi

                # Count workers
                local worker_count
                worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')
                info "  Active workers: $worker_count"

                # List workers
                if [[ "$worker_count" -gt 0 ]]; then
                    echo ""
                    info "Workers:"
                    docker ps --filter "name=hal9000-worker" --format '  {{.Names}}: {{.Status}}' 2>/dev/null
                fi
                ;;
            exited)
                warn "Parent container: Stopped"
                local exit_code
                exit_code=$(docker inspect --format '{{.State.ExitCode}}' "$PARENT_CONTAINER" 2>/dev/null || echo "?")
                info "  Exit code: $exit_code"
                ;;
            *)
                warn "Parent container: $status"
                ;;
        esac
    else
        warn "Parent container: Not found"
        info "  Run 'hal-9000 daemon start' to create"
    fi

    # Check for shared volumes
    echo ""
    info "Shared volumes:"
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if docker volume inspect "$vol" >/dev/null 2>&1; then
            success "  $vol: exists"
        else
            info "  $vol: not created"
        fi
    done
}

daemon_start() {
    info "Starting HAL-9000 orchestrator..."

    # Check if already running
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        success "Orchestrator already running"
        daemon_status
        return 0
    fi

    # Remove stopped container if exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped parent container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1 || true
    fi

    # Pull image if not available
    if ! docker image inspect "$PARENT_IMAGE" >/dev/null 2>&1; then
        info "Pulling parent image: $PARENT_IMAGE"
        docker pull "$PARENT_IMAGE" || {
            warn "Could not pull image, trying local build..."
            local docker_dir="${HAL9000_HOME}/../hal-9000/plugins/hal-9000/docker"
            if [[ -f "$docker_dir/Dockerfile.parent" ]]; then
                docker build -f "$docker_dir/Dockerfile.parent" -t "$PARENT_IMAGE" "$docker_dir"
            else
                error "Parent image not available. Build with: make build-parent" 3
            fi
        }
    fi

    # Create shared volumes if they don't exist
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if ! docker volume inspect "$vol" >/dev/null 2>&1; then
            info "Creating volume: $vol"
            docker volume create "$vol" >/dev/null
        fi
    done

    # Ensure hal9000 directories exist
    mkdir -p "$HAL9000_HOME/sessions"
    mkdir -p "$HAL9000_HOME/logs"
    mkdir -p "$HAL9000_HOME/config"

    # Start parent container
    info "Starting parent container..."
    local docker_args=(
        docker run -d
        --name "$PARENT_CONTAINER"
        --restart unless-stopped
        -v /var/run/docker.sock:/var/run/docker.sock
        -v "${HAL9000_HOME}:/root/.hal9000"
        -v hal9000-chromadb:/data/chromadb
        -v hal9000-memorybank:/data/membank
        -v hal9000-plugins:/data/plugins
    )

    # Pass through API key if set
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e ANTHROPIC_API_KEY)
    fi

    docker_args+=("$PARENT_IMAGE")

    if "${docker_args[@]}" >/dev/null; then
        # Wait for ChromaDB to start
        info "Waiting for ChromaDB server..."
        local max_wait=30
        local waited=0
        while [[ $waited -lt $max_wait ]]; do
            if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                success "ChromaDB server ready"
                break
            fi
            sleep 1
            ((waited++))
        done

        if [[ $waited -ge $max_wait ]]; then
            warn "ChromaDB server did not respond within ${max_wait}s"
        fi

        success "Orchestrator started"
        echo ""
        daemon_status
    else
        error "Failed to start parent container" 3
    fi
}

daemon_stop() {
    info "Stopping HAL-9000 orchestrator..."

    # Check for running workers
    local worker_count
    worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')

    if [[ "$worker_count" -gt 0 ]]; then
        warn "There are $worker_count active workers"
        info "Workers will lose their network connection to ChromaDB"
        echo ""
        read -p "Stop anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Cancelled"
            return 0
        fi
    fi

    # Stop parent container
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Stopping parent container..."
        docker stop "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Orchestrator stopped"
    elif docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Container removed"
    else
        info "Orchestrator not running"
    fi
}

daemon_restart() {
    info "Restarting HAL-9000 orchestrator..."
    daemon_stop
    sleep 2
    daemon_start
}

handle_daemon_command() {
    local cmd="${1:-}"

    case "$cmd" in
        start)
            daemon_start
            ;;
        stop)
            daemon_stop
            ;;
        status)
            daemon_status
            ;;
        restart)
            daemon_restart
            ;;
        ""|help)
            echo "Usage: hal-9000 daemon <command>"
            echo ""
            echo "Commands:"
            echo "  start    Start the orchestrator (parent container with ChromaDB server)"
            echo "  stop     Stop the orchestrator"
            echo "  status   Show orchestrator status and worker count"
            echo "  restart  Restart the orchestrator"
            ;;
        *)
            error "Unknown daemon command: $cmd" 2
            ;;
    esac
}

#==============================================================================
# Pool management
#==============================================================================

handle_pool_command() {
    local cmd="${1:-}"
    shift || true

    # Handle help without requiring parent
    if [[ "$cmd" == "help" ]] || [[ -z "$cmd" ]]; then
        echo "Usage: hal-9000 pool <command> [options]"
        echo ""
        echo "Manage the worker pool for fast container startup."
        echo ""
        echo "Commands:"
        echo "  start [--min-warm N] [--max-warm N]  Start the pool manager"
        echo "  stop                                  Stop the pool manager"
        echo "  status                                Show pool status"
        echo "  scale <n>                             Scale to n warm workers"
        echo "  cleanup                               Force cleanup idle workers"
        echo "  warm                                  Create a single warm worker"
        echo ""
        echo "Options:"
        echo "  --min-warm N      Minimum warm workers (default: 2)"
        echo "  --max-warm N      Maximum warm workers (default: 5)"
        echo "  --idle-timeout N  Seconds before cleanup (default: 300)"
        return 0
    fi

    # Check if parent is running for other commands
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        error "Parent container not running. Start with: hal-9000 daemon start" 3
    fi

    case "$cmd" in
        start)
            info "Starting pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh start "$@"
            ;;
        stop)
            info "Stopping pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh stop
            ;;
        status)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh status
            ;;
        scale)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh scale "$@"
            ;;
        cleanup)
            info "Cleaning up idle workers..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh cleanup
            ;;
        warm)
            info "Creating warm worker..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh warm
            ;;
        *)
            error "Unknown pool command: $cmd" 2
            ;;
    esac
}

#==============================================================================
# Via-parent launch (DinD mode)
#==============================================================================

launch_via_parent() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local detach="$6"
    local abs_project_path
    local image_base

    abs_project_path=$(cd "$project_dir" && pwd)
    image_base=$(get_container_image_base)

    # Check if parent is running
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        warn "Parent container not running"
        echo ""
        read -p "Start the orchestrator now? [Y/n] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Nn]$ ]]; then
            daemon_start
        else
            error "Cannot launch via parent without orchestrator. Run 'hal-9000 daemon start' first." 3
        fi
    fi

    info "Launching worker via parent container..."
    info "  Worker: $session_name"
    info "  Profile: $profile"
    info "  Project: $abs_project_path"

    # Build spawn-worker command
    local spawn_args=()
    spawn_args+=(-n "$session_name")

    if [[ "$detach" == "true" ]]; then
        spawn_args+=(-d)
    fi

    # Use worker image based on profile
    local worker_image="${image_base}:${profile}"
    spawn_args+=(-i "$worker_image")

    # Project directory - use HOST path because spawn-worker.sh
    # talks to the host's Docker daemon via socket
    spawn_args+=("$abs_project_path")

    # Execute spawn-worker.sh inside parent container
    # Pass through environment variables
    local exec_args=(docker exec)

    if [[ "$detach" != "true" ]]; then
        exec_args+=(-it)
    fi

    if [[ -n "$api_key" ]]; then
        exec_args+=(-e "ANTHROPIC_API_KEY=$api_key")
    elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        exec_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
    fi

    exec_args+=("$PARENT_CONTAINER" /scripts/spawn-worker.sh "${spawn_args[@]}")

    info "Executing: ${exec_args[*]}"

    if "${exec_args[@]}"; then
        if [[ "$detach" == "true" ]]; then
            success "Worker spawned in background: $session_name"
            info "Attach with: docker exec -it $session_name bash"
        fi
    else
        error "Failed to spawn worker via parent" 3
    fi
}

#==============================================================================
# Profile detection (See: hal-9000-installation-setup.md)
#==============================================================================

detect_profile() {
    local target_dir="${1:-.}"

    # Check for Java project
    if [[ -f "$target_dir/pom.xml" ]] || [[ -f "$target_dir/build.gradle" ]] || [[ -f "$target_dir/build.gradle.kts" ]]; then
        echo "java"
        return 0
    fi

    # Check for Python project
    if [[ -f "$target_dir/pyproject.toml" ]] || [[ -f "$target_dir/Pipfile" ]] || [[ -f "$target_dir/requirements.txt" ]]; then
        echo "python"
        return 0
    fi

    # Check for Node.js project
    if [[ -f "$target_dir/package.json" ]]; then
        echo "node"
        return 0
    fi

    # Default to base
    echo "$DEFAULT_PROFILE"
}

#==============================================================================
# Session management
#==============================================================================

get_session_name() {
    local project_dir="${1:-.}"
    local abs_path
    abs_path=$(cd "$project_dir" && pwd)

    # Hash the path to create deterministic session name
    # Use base64 to make it readable but unique
    local hash
    hash=$(echo -n "$abs_path" | shasum -a 256 | cut -c1-8)

    local basename
    basename=$(basename "$abs_path")

    # Clean basename for tmux (alphanumeric, hyphen, underscore only)
    basename=$(echo "$basename" | sed 's/[^a-zA-Z0-9_-]/-/g')

    echo "hal-9000-${basename}-${hash}"
}

verify_prerequisites() {
    local missing_tools=()

    # Check for required tools
    for tool in docker bash; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        fi
    done

    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        error "Missing required tools: ${missing_tools[*]}" 3
    fi

    # Check Docker daemon
    if ! docker ps &> /dev/null; then
        error "Docker daemon not running or not accessible" 3
    fi

    # Check authentication - API key OR subscription login
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        # No API key - check for subscription login
        if check_subscription_auth; then
            success "Authentication: Subscription login detected"
        else
            echo ""
            warn "No authentication configured"
            echo ""
            info "Option 1: Subscription login (recommended)"
            echo "  hal-9000 /login"
            echo ""
            info "Option 2: API key"
            echo "  export ANTHROPIC_API_KEY=sk-ant-api03-..."
            echo "  Get key at: https://console.anthropic.com/settings/keys"
            echo ""
            error "Run 'hal-9000 /login' to authenticate with your Claude subscription" 4
        fi
    else
        success "Authentication: API key configured"
    fi

    success "Prerequisites verified"
}

#==============================================================================
# Session initialization
#==============================================================================

init_session() {
    local project_dir="${1:-.}"
    local profile="${2:-$(detect_profile "$project_dir")}"
    local session_name="${3:-$(get_session_name "$project_dir")}"
    local shell_mode="${4:-false}"
    local abs_project_path
    local image_base

    abs_project_path=$(cd "$project_dir" && pwd)
    image_base=$(get_container_image_base)

    # Ensure shared Docker volumes exist
    ensure_volumes

    # Create session metadata directory
    local session_dir="$HAL9000_HOME/claude/$session_name"
    mkdir -p "$session_dir"

    # Create session metadata
    cat > "$session_dir/.hal-9000-session.json" << METADATA
{
  "name": "$session_name",
  "profile": "$profile",
  "project_dir": "$abs_project_path",
  "created_at": "$(date -Iseconds)",
  "shell_mode": $shell_mode,
  "container_image": "${image_base}:${profile}",
  "hal-9000_version": "$SCRIPT_VERSION"
}
METADATA

    echo "$session_name"
}

#==============================================================================
# Session management
#==============================================================================

# List running hal-9000 sessions
list_sessions() {
    info "Running hal-9000 sessions:"
    echo ""

    local sessions
    sessions=$(docker ps --filter "label=hal9000.session=true" --format "table {{.Names}}\t{{.Status}}\t{{.Label \"hal9000.project\"}}" 2>/dev/null)

    if [[ -z "$sessions" ]] || [[ "$sessions" == "NAMES"* && $(echo "$sessions" | wc -l) -eq 1 ]]; then
        info "  No running sessions"
        echo ""
        info "Start a new session with: hal-9000 /path/to/project"
    else
        echo "$sessions"
    fi
}

# Attach to an existing session
attach_session() {
    local session_name="${1:-}"

    if [[ -z "$session_name" ]]; then
        # If no session specified, list and pick
        local sessions
        sessions=$(docker ps --filter "label=hal9000.session=true" --format "{{.Names}}" 2>/dev/null)

        if [[ -z "$sessions" ]]; then
            error "No running sessions. Start one with: hal-9000 /path/to/project" 5
        fi

        local count
        count=$(echo "$sessions" | wc -l | tr -d ' ')

        if [[ "$count" -eq 1 ]]; then
            session_name="$sessions"
            info "Attaching to: $session_name"
        else
            info "Multiple sessions running. Specify one:"
            echo "$sessions" | while read -r s; do
                echo "  hal-9000 attach $s"
            done
            exit 1
        fi
    fi

    # Check if session exists
    if ! docker ps --format '{{.Names}}' | grep -q "^${session_name}$"; then
        error "Session not found: $session_name" 5
    fi

    # Get the shell_mode flag from the container labels
    local shell_mode
    shell_mode=$(docker inspect --format='{{index .Config.Labels "hal9000.shell_mode"}}' "$session_name" 2>/dev/null || echo "false")

    info "Attaching to session: $session_name"
    echo ""

    if [[ "$shell_mode" == "true" ]]; then
        # Shell mode: just attach to bash
        info "Type 'exit' to disconnect"
        docker exec -it "$session_name" bash
    else
        # Normal mode: restore session state and run Claude with proper TTY
        info "Launching Claude..."
        docker exec -it "$session_name" bash -c "
            # Restore Claude session state from persistent volume
            if [ -f /root/.claude-session/.claude.json ]; then
              cp /root/.claude-session/.claude.json /root/.claude.json
            fi
            # Run Claude
            claude
            # Save Claude session state back to persistent volume after exit
            [ -f /root/.claude.json ] && cp /root/.claude.json /root/.claude-session/.claude.json
        "
    fi
}

# Kill a session
kill_session() {
    local session_name="${1:-}"

    if [[ -z "$session_name" ]]; then
        error "Usage: hal-9000 kill <session-name>" 2
    fi

    if ! docker ps --format '{{.Names}}' | grep -q "^${session_name}$"; then
        error "Session not found: $session_name" 5
    fi

    info "Stopping session: $session_name"
    docker stop "$session_name" >/dev/null 2>&1
    docker rm "$session_name" >/dev/null 2>&1
    success "Session stopped: $session_name"
}

#==============================================================================
# Container launching
#==============================================================================

launch_container_session() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local detach="${6:-false}"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)

    # Determine container image: local profile > remote profile > base
    local image_base=$(get_container_image_base)
    local container_image="${image_base}:${profile}"

    # Check for local profile and build if needed
    if local_image=$(get_local_profile_image "$profile" 2>/dev/null); then
        # Local profile exists - build it if not already built
        build_local_profile "$profile"
        container_image="$local_image"
        info "Using local profile: $profile"
    fi

    # Check if session already exists
    local session_exists=false
    if docker ps --format '{{.Names}}' | grep -q "^${session_name}$"; then
        session_exists=true
        info "Session already running: $session_name"
        if [[ "$detach" == "true" ]]; then
            info "Use 'hal-9000 attach $session_name' to attach"
            return 0
        else
            # Reuse existing session (will continue to cleanup code below if interactive)
            shell_mode=$(docker inspect --format='{{index .Config.Labels "hal9000.shell_mode"}}' "$session_name" 2>/dev/null || echo "false")
        fi
    fi

    # Verify image exists or fall back to base
    if ! docker inspect "$container_image" &> /dev/null; then
        warn "Image not found: $container_image, using base profile"
        container_image="${image_base}:base"
    fi

    info "Launching Claude in container..."
    info "  Profile: $profile"
    info "  Session: $session_name"
    info "  Directory: $abs_project_path"

    # Build docker run command
    # NO --rm: container persists for reattachment
    # Uses shared Docker volumes for CLAUDE_HOME and memory bank
    local docker_args=(
        docker run
        -d
        --name "$session_name"
        --label "hal9000.session=true"
        --label "hal9000.project=$abs_project_path"
        --label "hal9000.profile=$profile"
        --label "hal9000.shell_mode=$shell_mode"
        -v "$abs_project_path:/workspace"
        -v "${CLAUDE_HOME_VOLUME}:/root/.claude"
        -v "${CLAUDE_SESSION_VOLUME}:/root/.claude-session"
        -v "${MEMORY_BANK_VOLUME}:/root/memory-bank"
        -v /var/run/docker.sock:/var/run/docker.sock
        -e "CLAUDE_HOME=/root/.claude"
        -e "HAL9000_SESSION=$session_name"
        -e "HAL9000_PROJECT_DIR=$abs_project_path"
        -e "MEMORY_BANK_ROOT=/root/memory-bank"
    )

    # Pass through API key if available
    if [[ -n "$api_key" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$api_key")
        info "  Auth: API key"
    elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
        info "  Auth: API key (from environment)"
    else
        info "  Auth: Subscription (from volume) or interactive login"
    fi

    # Add image and startup command
    docker_args+=(-w "/workspace" "$container_image")

    # Start container with Claude or shell
    # Container keeps running with bash so user can access shell afterward
    # Key: Restore .claude.json from persistent session volume before running Claude
    # Container startup: just keep it alive indefinitely
    # Claude will be launched via docker exec, which provides proper TTY
    docker_args+=(bash -c "
        # Keep container alive for docker exec to attach
        # User will interact via 'docker exec -it' which provides the TTY
        tail -f /dev/null
    ")

    # Start the container (unless it already exists)
    if [[ "$session_exists" != "true" ]]; then
        info "Starting container..."
        if ! "${docker_args[@]}" >/dev/null; then
            error "Failed to start container" 3
        fi
    fi

    if [[ "$detach" == "true" ]]; then
        # Detached mode: don't attach, just report
        if [[ "$session_exists" == "true" ]]; then
            success "Session already running in background: $session_name"
        else
            success "Session started in background: $session_name"
        fi
        echo ""
        info "Attach with: hal-9000 attach $session_name"
        info "Or simply: hal-9000 attach (if only one session)"
    else
        # Interactive mode: attach and clean up when user exits
        if [[ "$session_exists" == "true" ]]; then
            info "Reattaching to session: $session_name"
        else
            info "Attaching to session: $session_name"
        fi
        echo ""

        if [[ "$shell_mode" == "true" ]]; then
            # Shell mode: just attach to bash
            info "Type 'exit' to disconnect"
            docker exec -it "$session_name" bash
        else
            # Normal mode: restore session state and run Claude with proper TTY
            info "Launching Claude..."
            docker exec -it "$session_name" bash -c "
                # Restore Claude session state from persistent volume
                # The volume is initialized with pristine config on first creation,
                # so the container gets a clean environment with no host-specific paths
                if [ -f /root/.claude-session/.claude.json ]; then
                  cp /root/.claude-session/.claude.json /root/.claude.json
                fi
                # Run Claude
                claude
                # Save Claude session state back to persistent volume after exit
                [ -f /root/.claude.json ] && cp /root/.claude.json /root/.claude-session/.claude.json
            "
        fi

        # Clean up container after interactive session ends
        info "Cleaning up container..."
        docker stop "$session_name" >/dev/null 2>&1
        docker rm "$session_name" >/dev/null 2>&1
        success "Session ended: $session_name"
    fi
}

#==============================================================================
# Main entry point
#==============================================================================

main() {
    local project_dir="."
    local profile=""
    local session_name=""
    local api_key=""
    local shell_mode=false
    local detach=false
    local verify_only=false
    local via_parent=false

    # Ensure volumes exist and are initialized with pristine config
    # This runs once on first use, subsequent launches reuse initialized volumes
    ensure_volumes

    # Handle daemon subcommand first
    if [[ "${1:-}" == "daemon" ]]; then
        shift
        handle_daemon_command "$@"
        exit 0
    fi

    # Handle pool subcommand
    if [[ "${1:-}" == "pool" ]]; then
        shift
        handle_pool_command "$@"
        exit 0
    fi

    # Handle session management subcommands
    if [[ "${1:-}" == "sessions" ]]; then
        list_sessions
        exit 0
    fi

    if [[ "${1:-}" == "attach" ]]; then
        shift
        attach_session "$@"
        exit 0
    fi

    if [[ "${1:-}" == "kill" ]]; then
        shift
        kill_session "$@"
        exit 0
    fi

    if [[ "${1:-}" == "profiles" ]]; then
        shift
        case "${1:-}" in
            build)
                if [[ -z "${2:-}" ]]; then
                    error "Usage: hal-9000 profiles build <profile-name>" 2
                fi
                build_local_profile "$2"
                exit 0
                ;;
            list|"")
                list_local_profiles
                exit 0
                ;;
            *)
                error "Unknown profiles command: $1" 2
                ;;
        esac
    fi

    # Handle claude subcommand passthrough (plugin, mcp, doctor, etc.)
    if is_claude_subcommand "${1:-}"; then
        run_claude_command "$@"
        exit $?
    fi

    # Parse arguments for interactive session mode
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --profile)
                profile="$2"
                shift 2
                ;;
            --shell|-s)
                shell_mode=true
                shift
                ;;
            --name)
                session_name="$2"
                shift 2
                ;;
            --api-key)
                api_key="$2"
                shift 2
                ;;
            --detach|-d)
                detach=true
                shift
                ;;
            --via-parent)
                via_parent=true
                shift
                ;;
            --verify)
                verify_only=true
                shift
                ;;
            --setup)
                setup_auth
                exit 0
                ;;
            --diagnose)
                show_diagnostics
                exit 0
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            --version|-v)
                show_version
                exit 0
                ;;
            -*)
                error "Unknown option: $1" 2
                ;;
            *)
                project_dir="$1"
                shift
                ;;
        esac
    done

    # Verify prerequisites (skip API key check for via-parent mode)
    if [[ "$via_parent" == "true" ]]; then
        # For via-parent, just check Docker is available
        if ! command -v docker &> /dev/null; then
            error "Docker not found" 3
        fi
        if ! docker ps &> /dev/null; then
            error "Docker daemon not running" 3
        fi
    else
        if ! verify_prerequisites; then
            exit 1
        fi
    fi

    if [[ "$verify_only" == "true" ]]; then
        exit 0
    fi

    # Change to project directory
    if [[ ! -d "$project_dir" ]]; then
        error "Directory not found: $project_dir" 1
    fi

    # Auto-detect profile if not specified
    if [[ -z "$profile" ]]; then
        profile=$(detect_profile "$project_dir")
        info "Auto-detected profile: $profile"
    fi

    # Initialize session
    if [[ -z "$session_name" ]]; then
        session_name=$(get_session_name "$project_dir")
    fi

    # Launch via parent or direct
    if [[ "$via_parent" == "true" ]]; then
        # DinD mode: spawn worker via parent container
        info "Using via-parent mode (DinD)"
        launch_via_parent "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" "$detach" || error "Failed to launch via parent" 3
    else
        # Direct mode: launch container directly
        info "Initializing session..."
        init_session "$project_dir" "$profile" "$session_name" "$shell_mode" || error "Failed to initialize session" 1

        success "Session ready: $session_name"

        # Launch container
        launch_container_session "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" "$detach" || error "Failed to launch container" 3
    fi
}

# Run main if script is executed directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
