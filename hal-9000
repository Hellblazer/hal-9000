#!/bin/bash
# hal-9000 - Containerized Claude CLI launcher
# Opens Claude inside a tmux session within a hal-9000 container

set -Eeuo pipefail

readonly SCRIPT_NAME="hal-9000"
readonly SCRIPT_VERSION="0.7.0"
readonly HAL9000_HOME="${HOME}/.hal9000"
readonly PARENT_CONTAINER="hal9000-parent"
readonly PARENT_IMAGE="ghcr.io/hellblazer/hal-9000:parent"
readonly DEFAULT_PROFILE="base"
readonly CONTAINER_IMAGE_BASE="ghcr.io/hellblazer/hal-9000"

# Shared Docker volume for CLAUDE_HOME - all workers share this
readonly CLAUDE_HOME_VOLUME="hal9000-claude-home"
readonly MEMORY_BANK_VOLUME="hal9000-memory-bank"

# Claude subcommands that should be passed through to container
readonly CLAUDE_SUBCOMMANDS="mcp|plugin|doctor|install|setup-token|update"

# Colors for output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m' # No Color

#==============================================================================
# Error handling
#==============================================================================

error() {
    echo -e "${RED}✗ Error: $*${NC}" >&2
    exit 1
}

warn() {
    echo -e "${YELLOW}⚠ Warning: $*${NC}" >&2
}

info() {
    echo -e "${BLUE}ℹ $*${NC}"
}

success() {
    echo -e "${GREEN}✓ $*${NC}"
}

#==============================================================================
# Claude command passthrough
#==============================================================================

# Ensure shared volumes exist
ensure_volumes() {
    docker volume create "$CLAUDE_HOME_VOLUME" >/dev/null 2>&1 || true
    docker volume create "$MEMORY_BANK_VOLUME" >/dev/null 2>&1 || true
}

# Check if subscription login is configured in the shared volume
# Returns 0 if logged in, 1 if not
check_subscription_auth() {
    ensure_volumes

    # Check for auth credentials in the volume
    # Claude stores session data in .claude/
    docker run --rm \
        -v "${CLAUDE_HOME_VOLUME}:/root/.claude:ro" \
        alpine:latest \
        sh -c 'test -f /root/.claude/.credentials.json || test -f /root/.claude/statsig_user_id' \
        2>/dev/null
}

# Open URL in browser (cross-platform)
open_url() {
    local url="$1"
    if command -v open &>/dev/null; then
        open "$url" 2>/dev/null  # macOS
    elif command -v xdg-open &>/dev/null; then
        xdg-open "$url" 2>/dev/null  # Linux
    elif command -v wslview &>/dev/null; then
        wslview "$url" 2>/dev/null  # WSL
    fi
}

# Run /login with auto-browser opening
# Uses 'script' to preserve TTY while capturing output for URL detection
run_login_command() {
    ensure_volumes

    local image="${CONTAINER_IMAGE_BASE}:base"
    local output_file
    output_file=$(mktemp)

    info "Starting login... (browser will open automatically if URL detected)"

    # Background watcher for URLs
    (
        # Wait for file to exist and have content
        sleep 1
        tail -f "$output_file" 2>/dev/null | while IFS= read -r line; do
            # Look for anthropic/claude URLs
            if [[ "$line" =~ (https://[^[:space:]\"\']+anthropic[^[:space:]\"\']*) ]] || \
               [[ "$line" =~ (https://[^[:space:]\"\']+claude[^[:space:]\"\']*) ]]; then
                local url="${BASH_REMATCH[1]}"
                # Clean up any trailing characters
                url="${url%%[\"\']*}"
                success "Opening browser: $url"
                open_url "$url"
            fi
        done
    ) &
    local watcher_pid=$!

    # Build docker command
    local docker_cmd="docker run --rm -it \
        -v ${CLAUDE_HOME_VOLUME}:/root/.claude \
        -v ${MEMORY_BANK_VOLUME}:/root/memory-bank \
        -e MEMORY_BANK_ROOT=/root/memory-bank \
        $image claude /login"

    # Use 'script' to preserve TTY while logging output
    # macOS and Linux have different syntax
    if [[ "$(uname)" == "Darwin" ]]; then
        script -q "$output_file" bash -c "$docker_cmd"
    else
        script -q -c "$docker_cmd" "$output_file"
    fi

    local exit_code=$?

    # Cleanup
    kill "$watcher_pid" 2>/dev/null || true
    rm -f "$output_file"

    return $exit_code
}

# Run a claude command inside a container with shared volumes
# Usage: run_claude_command <args...>
run_claude_command() {
    # Special handling for /login to auto-open browser
    if [[ "${1:-}" == "/login" ]]; then
        run_login_command
        return $?
    fi

    ensure_volumes

    local image="${CONTAINER_IMAGE_BASE}:base"

    # Build docker args
    local docker_args=(
        docker run --rm -it
        -v "${CLAUDE_HOME_VOLUME}:/root/.claude"
        -v "${MEMORY_BANK_VOLUME}:/root/memory-bank"
        -e "MEMORY_BANK_ROOT=/root/memory-bank"
    )

    # Pass through API key if available
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
    fi

    # Run claude with the provided arguments
    "${docker_args[@]}" "$image" claude "$@"
}

# Check if arg is a claude subcommand that should be passed through
is_claude_subcommand() {
    local arg="${1:-}"
    # Match known subcommands (mcp, plugin, doctor, etc.)
    [[ "$arg" =~ ^($CLAUDE_SUBCOMMANDS)$ ]] && return 0
    # Match Claude slash commands like /login, /help, /status
    [[ "$arg" =~ ^/ ]] && [[ ! -d "$arg" ]] && return 0
    return 1
}

#==============================================================================
# Help and diagnostics
#==============================================================================

show_help() {
    cat << 'EOF'
hal-9000 - Containerized Claude CLI

USAGE:
  hal-9000 [OPTIONS] [DIRECTORY]           # Interactive Claude session
  hal-9000 <claude-command> [args...]      # Run claude command in container
  hal-9000 daemon <command>                # Manage orchestrator
  hal-9000 pool <command>                  # Manage worker pool

CLAUDE COMMANDS (passthrough to container):
  hal-9000 plugin list                     # List installed plugins
  hal-9000 plugin install <name>           # Install a plugin
  hal-9000 plugin marketplace add <url>    # Add a marketplace
  hal-9000 mcp list                        # List MCP servers
  hal-9000 mcp add <server>                # Add MCP server
  hal-9000 doctor                          # Check Claude health

  All 'claude' subcommands work - they run inside a container with the
  shared CLAUDE_HOME volume, so changes persist across all sessions.

DAEMON COMMANDS:
  hal-9000 daemon start    Start the orchestrator (parent container with ChromaDB)
  hal-9000 daemon stop     Stop the orchestrator
  hal-9000 daemon status   Show orchestrator status and worker count
  hal-9000 daemon restart  Restart the orchestrator

POOL COMMANDS:
  hal-9000 pool start      Start the worker pool manager
  hal-9000 pool stop       Stop the worker pool manager
  hal-9000 pool status     Show pool status (warm/busy workers)
  hal-9000 pool scale <n>  Scale to n warm workers
  hal-9000 pool cleanup    Force cleanup of idle workers

OPTIONS:
  --setup                Interactive setup for API key (first-time setup)
  --profile PROFILE      Profile: base, python, node, java (default: auto-detect)
  --shell, -s            Start bash shell instead of Claude
  --name NAME            Custom session name (default: auto from directory)
  --api-key KEY          Anthropic API key (or set ANTHROPIC_API_KEY env var)
  --detach, -d           Don't attach to tmux session after launch
  --via-parent           Launch worker via parent container (requires daemon running)
  --verify               Verify prerequisites and exit
  --diagnose             Show diagnostic information
  --help, -h             Show this help message
  --version, -v          Show version

ARGUMENTS:
  DIRECTORY              Project directory (default: current directory)

AUTHENTICATION:
  Option 1 - Subscription Login (recommended):
    hal-9000 /login      # Login once, persists for all sessions

  Option 2 - API Key:
    export ANTHROPIC_API_KEY=sk-ant-api03-...
    Get key at: https://console.anthropic.com/settings/keys
    Add to ~/.bashrc or ~/.zshrc to persist.

  Auth is stored in shared Docker volume - login once, use everywhere.

EXAMPLES:
  hal-9000                                    # Current dir, auto-detect profile
  hal-9000 --profile python                   # Force Python profile
  hal-9000 ~/projects/myapp                   # Specific directory
  hal-9000 --shell -d                         # Detached shell session
  hal-9000 plugin install memory-bank         # Install plugin (persists)
  hal-9000 mcp list                           # List MCP servers
  hal-9000 --diagnose                         # Check setup

DAEMON EXAMPLES:
  hal-9000 daemon start                       # Start the orchestrator
  hal-9000 daemon status                      # Check status and workers
  hal-9000 daemon stop                        # Stop the orchestrator
  hal-9000 --via-parent ~/project             # Launch worker via parent (DinD mode)

ENVIRONMENT:
  ANTHROPIC_API_KEY      API key for Claude authentication
  HAL9000_HOME           Override session storage (default: ~/.hal9000)
  DOCKER_SOCKET          Docker socket path (default: /var/run/docker.sock)

VOLUMES:
  hal9000-claude-home    Shared CLAUDE_HOME - plugins/config persist here
  hal9000-memory-bank    Shared memory bank for cross-session context

For more information, see documentation at:
  https://github.com/hellblazer/hal-9000/blob/main/README.md
EOF
}

show_version() {
    echo "hal-9000 version $SCRIPT_VERSION"
    echo "hal-9000 plugin version 1.4.0+"
}

setup_auth() {
    echo ""
    info "hal-9000 Authentication Setup"
    echo ""

    # Check if already configured
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        success "ANTHROPIC_API_KEY is already set"
        echo "  Current value: ${ANTHROPIC_API_KEY:0:15}..."
        echo ""
        read -p "Replace with a new key? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Keeping existing configuration"
            exit 0
        fi
    fi

    echo "Enter your Anthropic API key (or 'q' to quit):"
    echo "  Get one at: https://console.anthropic.com/settings/keys"
    echo ""
    read -p "API Key: " api_key_input

    if [[ "$api_key_input" == "q" ]] || [[ -z "$api_key_input" ]]; then
        info "Setup cancelled"
        exit 0
    fi

    # Validate format
    if [[ ! "$api_key_input" =~ ^sk-ant- ]]; then
        warn "Key doesn't look like an Anthropic API key (expected sk-ant-...)"
        read -p "Continue anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            exit 1
        fi
    fi

    # Detect shell config file
    local shell_rc=""
    if [[ -n "${ZSH_VERSION:-}" ]] || [[ "$SHELL" == *"zsh"* ]]; then
        shell_rc="$HOME/.zshrc"
    else
        shell_rc="$HOME/.bashrc"
    fi

    echo ""
    info "Where to save the API key?"
    echo "  1) $shell_rc (recommended - persists across sessions)"
    echo "  2) Current session only (export now, won't persist)"
    echo "  3) Cancel"
    echo ""
    read -p "Choice [1]: " -n 1 -r choice
    echo ""

    case "${choice:-1}" in
        1)
            # Check if already in file
            if grep -q "ANTHROPIC_API_KEY" "$shell_rc" 2>/dev/null; then
                warn "ANTHROPIC_API_KEY already exists in $shell_rc"
                read -p "Replace it? [y/N] " -n 1 -r
                echo ""
                if [[ $REPLY =~ ^[Yy]$ ]]; then
                    # Remove old line and add new
                    sed -i.bak '/export ANTHROPIC_API_KEY/d' "$shell_rc"
                else
                    info "Setup cancelled"
                    exit 0
                fi
            fi

            echo "" >> "$shell_rc"
            echo "# Anthropic API key for Claude Code (added by hal-9000)" >> "$shell_rc"
            echo "export ANTHROPIC_API_KEY=\"$api_key_input\"" >> "$shell_rc"

            success "Added to $shell_rc"
            echo ""
            info "To use now, run:"
            echo "  source $shell_rc"
            echo ""
            info "Or start a new terminal session."
            ;;
        2)
            export ANTHROPIC_API_KEY="$api_key_input"
            success "API key set for current session"
            echo ""
            info "Note: This won't persist. Run 'hal-9000 --setup' again to save permanently."
            ;;
        *)
            info "Setup cancelled"
            exit 0
            ;;
    esac

    echo ""
    success "Setup complete! You can now run: hal-9000"
}

show_diagnostics() {
    info "hal-9000 Diagnostics"
    echo ""

    # Check Docker
    if command -v docker &> /dev/null; then
        DOCKER_VERSION=$(docker --version)
        success "Docker: $DOCKER_VERSION"
    else
        error "Docker not found. Install Docker to use hal-9000."
    fi

    # Check tmux
    if command -v tmux &> /dev/null; then
        TMUX_VERSION=$(tmux -V)
        success "tmux: $TMUX_VERSION"
    else
        warn "tmux not found. Install tmux for full functionality."
    fi

    # Check bash version
    BASH_VERSION="${BASH_VERSION%.*}"
    if [[ "${BASH_VERSION}" =~ ^[5-9]|[0-9]{2,} ]]; then
        success "Bash: $BASH_VERSION (OK)"
    else
        warn "Bash version: $BASH_VERSION (recommend 5.0+)"
    fi

    # Check authentication
    echo ""
    info "Authentication:"
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        local key_preview="${ANTHROPIC_API_KEY:0:10}..."
        success "  API Key: Set ($key_preview)"
    else
        info "  API Key: Not set"
    fi

    if check_subscription_auth; then
        success "  Subscription: Logged in (credentials in volume)"
    else
        info "  Subscription: Not logged in"
        info "    → Run 'hal-9000 /login' to authenticate"
    fi

    # Check Docker volumes
    echo ""
    info "Docker Volumes:"
    if docker volume inspect "$CLAUDE_HOME_VOLUME" &>/dev/null; then
        success "  $CLAUDE_HOME_VOLUME (exists)"
        # Show volume size if possible
        local vol_path
        vol_path=$(docker volume inspect "$CLAUDE_HOME_VOLUME" --format '{{.Mountpoint}}' 2>/dev/null || true)
        if [[ -n "$vol_path" ]]; then
            info "    → Mountpoint: $vol_path"
        fi
    else
        info "  $CLAUDE_HOME_VOLUME (not yet created)"
    fi

    if docker volume inspect "$MEMORY_BANK_VOLUME" &>/dev/null; then
        success "  $MEMORY_BANK_VOLUME (exists)"
    else
        info "  $MEMORY_BANK_VOLUME (not yet created)"
    fi

    # Check hal9000 storage
    echo ""
    if [[ -d "$HAL9000_HOME" ]]; then
        SESSION_COUNT=$(find "$HAL9000_HOME/claude" -maxdepth 1 -type d 2>/dev/null | wc -l)
        success "hal9000 home: $HAL9000_HOME"
        info "  → Active sessions: $((SESSION_COUNT - 1))"
    else
        info "hal9000 home: $HAL9000_HOME (not yet created)"
    fi

    # Check Docker socket
    if [[ -S /var/run/docker.sock ]]; then
        success "Docker socket: /var/run/docker.sock"
    elif [[ -S "$HOME/.docker/run/docker.sock" ]]; then
        success "Docker socket: $HOME/.docker/run/docker.sock"
    else
        warn "Docker socket not found"
    fi

    # Check for container image
    echo ""
    if docker inspect "${CONTAINER_IMAGE_BASE}:base" &> /dev/null; then
        success "Container image: ${CONTAINER_IMAGE_BASE}:base (available)"
    else
        warn "Container image: ${CONTAINER_IMAGE_BASE}:base (not found - run: make build-base)"
    fi

    echo ""
    info "System: $(uname -s) $(uname -m)"
    info "Current directory: $(pwd)"

    # Auth guidance
    echo ""
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        info "Authentication options:"
        echo "  1. Set ANTHROPIC_API_KEY environment variable"
        echo "  2. Run 'hal-9000 --shell' then 'claude /login' inside container"
        echo "  3. Run 'claude setup-token' on host for long-lived token"
    fi

    # Container config info
    echo ""
    info "Container config:"
    echo "  CLAUDE_HOME: /root/.claude (from $CLAUDE_HOME_VOLUME volume)"
    echo "  Memory Bank: /root/memory-bank (from $MEMORY_BANK_VOLUME volume)"
    echo "  Plugins installed in container persist across all sessions"
}

#==============================================================================
# Daemon commands
#==============================================================================

daemon_status() {
    info "HAL-9000 Orchestrator Status"
    echo ""

    # Check if parent container exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        local status
        status=$(docker inspect --format '{{.State.Status}}' "$PARENT_CONTAINER" 2>/dev/null || echo "unknown")

        case "$status" in
            running)
                success "Parent container: Running"

                # Get uptime
                local started_at
                started_at=$(docker inspect --format '{{.State.StartedAt}}' "$PARENT_CONTAINER" 2>/dev/null || echo "")
                if [[ -n "$started_at" ]]; then
                    info "  Started: $started_at"
                fi

                # Check ChromaDB server
                if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                    success "  ChromaDB server: Running on port 8000"
                else
                    warn "  ChromaDB server: Not responding"
                fi

                # Count workers
                local worker_count
                worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')
                info "  Active workers: $worker_count"

                # List workers
                if [[ "$worker_count" -gt 0 ]]; then
                    echo ""
                    info "Workers:"
                    docker ps --filter "name=hal9000-worker" --format '  {{.Names}}: {{.Status}}' 2>/dev/null
                fi
                ;;
            exited)
                warn "Parent container: Stopped"
                local exit_code
                exit_code=$(docker inspect --format '{{.State.ExitCode}}' "$PARENT_CONTAINER" 2>/dev/null || echo "?")
                info "  Exit code: $exit_code"
                ;;
            *)
                warn "Parent container: $status"
                ;;
        esac
    else
        warn "Parent container: Not found"
        info "  Run 'hal-9000 daemon start' to create"
    fi

    # Check for shared volumes
    echo ""
    info "Shared volumes:"
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if docker volume inspect "$vol" >/dev/null 2>&1; then
            success "  $vol: exists"
        else
            info "  $vol: not created"
        fi
    done
}

daemon_start() {
    info "Starting HAL-9000 orchestrator..."

    # Check if already running
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        success "Orchestrator already running"
        daemon_status
        return 0
    fi

    # Remove stopped container if exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped parent container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1 || true
    fi

    # Pull image if not available
    if ! docker image inspect "$PARENT_IMAGE" >/dev/null 2>&1; then
        info "Pulling parent image: $PARENT_IMAGE"
        docker pull "$PARENT_IMAGE" || {
            warn "Could not pull image, trying local build..."
            local docker_dir="${HAL9000_HOME}/../hal-9000/plugins/hal-9000/docker"
            if [[ -f "$docker_dir/Dockerfile.parent" ]]; then
                docker build -f "$docker_dir/Dockerfile.parent" -t "$PARENT_IMAGE" "$docker_dir"
            else
                error "Parent image not available. Build with: make build-parent"
            fi
        }
    fi

    # Create shared volumes if they don't exist
    for vol in hal9000-chromadb hal9000-memorybank hal9000-plugins; do
        if ! docker volume inspect "$vol" >/dev/null 2>&1; then
            info "Creating volume: $vol"
            docker volume create "$vol" >/dev/null
        fi
    done

    # Ensure hal9000 directories exist
    mkdir -p "$HAL9000_HOME/sessions"
    mkdir -p "$HAL9000_HOME/logs"
    mkdir -p "$HAL9000_HOME/config"

    # Start parent container
    info "Starting parent container..."
    local docker_args=(
        docker run -d
        --name "$PARENT_CONTAINER"
        --restart unless-stopped
        -v /var/run/docker.sock:/var/run/docker.sock
        -v "${HAL9000_HOME}:/root/.hal9000"
        -v hal9000-chromadb:/data/chromadb
        -v hal9000-memorybank:/data/membank
        -v hal9000-plugins:/data/plugins
    )

    # Pass through API key if set
    if [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e ANTHROPIC_API_KEY)
    fi

    docker_args+=("$PARENT_IMAGE")

    if "${docker_args[@]}" >/dev/null; then
        # Wait for ChromaDB to start
        info "Waiting for ChromaDB server..."
        local max_wait=30
        local waited=0
        while [[ $waited -lt $max_wait ]]; do
            if docker exec "$PARENT_CONTAINER" curl -s "http://localhost:8000/api/v2/heartbeat" >/dev/null 2>&1; then
                success "ChromaDB server ready"
                break
            fi
            sleep 1
            ((waited++))
        done

        if [[ $waited -ge $max_wait ]]; then
            warn "ChromaDB server did not respond within ${max_wait}s"
        fi

        success "Orchestrator started"
        echo ""
        daemon_status
    else
        error "Failed to start parent container"
    fi
}

daemon_stop() {
    info "Stopping HAL-9000 orchestrator..."

    # Check for running workers
    local worker_count
    worker_count=$(docker ps --filter "name=hal9000-worker" --format '{{.Names}}' | wc -l | tr -d ' ')

    if [[ "$worker_count" -gt 0 ]]; then
        warn "There are $worker_count active workers"
        info "Workers will lose their network connection to ChromaDB"
        echo ""
        read -p "Stop anyway? [y/N] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            info "Cancelled"
            return 0
        fi
    fi

    # Stop parent container
    if docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Stopping parent container..."
        docker stop "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Orchestrator stopped"
    elif docker ps -a --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        info "Removing stopped container..."
        docker rm "$PARENT_CONTAINER" >/dev/null 2>&1
        success "Container removed"
    else
        info "Orchestrator not running"
    fi
}

daemon_restart() {
    info "Restarting HAL-9000 orchestrator..."
    daemon_stop
    sleep 2
    daemon_start
}

handle_daemon_command() {
    local cmd="${1:-}"

    case "$cmd" in
        start)
            daemon_start
            ;;
        stop)
            daemon_stop
            ;;
        status)
            daemon_status
            ;;
        restart)
            daemon_restart
            ;;
        ""|help)
            echo "Usage: hal-9000 daemon <command>"
            echo ""
            echo "Commands:"
            echo "  start    Start the orchestrator (parent container with ChromaDB server)"
            echo "  stop     Stop the orchestrator"
            echo "  status   Show orchestrator status and worker count"
            echo "  restart  Restart the orchestrator"
            ;;
        *)
            error "Unknown daemon command: $cmd"
            ;;
    esac
}

#==============================================================================
# Pool management
#==============================================================================

handle_pool_command() {
    local cmd="${1:-}"
    shift || true

    # Handle help without requiring parent
    if [[ "$cmd" == "help" ]] || [[ -z "$cmd" ]]; then
        echo "Usage: hal-9000 pool <command> [options]"
        echo ""
        echo "Manage the worker pool for fast container startup."
        echo ""
        echo "Commands:"
        echo "  start [--min-warm N] [--max-warm N]  Start the pool manager"
        echo "  stop                                  Stop the pool manager"
        echo "  status                                Show pool status"
        echo "  scale <n>                             Scale to n warm workers"
        echo "  cleanup                               Force cleanup idle workers"
        echo "  warm                                  Create a single warm worker"
        echo ""
        echo "Options:"
        echo "  --min-warm N      Minimum warm workers (default: 2)"
        echo "  --max-warm N      Maximum warm workers (default: 5)"
        echo "  --idle-timeout N  Seconds before cleanup (default: 300)"
        return 0
    fi

    # Check if parent is running for other commands
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        error "Parent container not running. Start with: hal-9000 daemon start"
    fi

    case "$cmd" in
        start)
            info "Starting pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh start "$@"
            ;;
        stop)
            info "Stopping pool manager..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh stop
            ;;
        status)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh status
            ;;
        scale)
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh scale "$@"
            ;;
        cleanup)
            info "Cleaning up idle workers..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh cleanup
            ;;
        warm)
            info "Creating warm worker..."
            docker exec "$PARENT_CONTAINER" /scripts/pool-manager.sh warm
            ;;
        *)
            error "Unknown pool command: $cmd"
            ;;
    esac
}

#==============================================================================
# Via-parent launch (DinD mode)
#==============================================================================

launch_via_parent() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local detach="$6"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)

    # Check if parent is running
    if ! docker ps --format '{{.Names}}' | grep -q "^${PARENT_CONTAINER}$"; then
        warn "Parent container not running"
        echo ""
        read -p "Start the orchestrator now? [Y/n] " -n 1 -r
        echo ""
        if [[ ! $REPLY =~ ^[Nn]$ ]]; then
            daemon_start
        else
            error "Cannot launch via parent without orchestrator. Run 'hal-9000 daemon start' first."
        fi
    fi

    info "Launching worker via parent container..."
    info "  Worker: $session_name"
    info "  Profile: $profile"
    info "  Project: $abs_project_path"

    # Build spawn-worker command
    local spawn_args=()
    spawn_args+=(-n "$session_name")

    if [[ "$detach" == "true" ]]; then
        spawn_args+=(-d)
    fi

    # Use worker image based on profile
    local worker_image="${CONTAINER_IMAGE_BASE}:${profile}"
    spawn_args+=(-i "$worker_image")

    # Project directory - use HOST path because spawn-worker.sh
    # talks to the host's Docker daemon via socket
    spawn_args+=("$abs_project_path")

    # Execute spawn-worker.sh inside parent container
    # Pass through environment variables
    local exec_args=(docker exec)

    if [[ "$detach" != "true" ]]; then
        exec_args+=(-it)
    fi

    if [[ -n "$api_key" ]]; then
        exec_args+=(-e "ANTHROPIC_API_KEY=$api_key")
    elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        exec_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
    fi

    exec_args+=("$PARENT_CONTAINER" /scripts/spawn-worker.sh "${spawn_args[@]}")

    info "Executing: ${exec_args[*]}"

    if "${exec_args[@]}"; then
        if [[ "$detach" == "true" ]]; then
            success "Worker spawned in background: $session_name"
            info "Attach with: docker exec -it $session_name bash"
        fi
    else
        error "Failed to spawn worker via parent"
    fi
}

#==============================================================================
# Profile detection (See: hal-9000-installation-setup.md)
#==============================================================================

detect_profile() {
    local target_dir="${1:-.}"

    # Check for Java project
    if [[ -f "$target_dir/pom.xml" ]] || [[ -f "$target_dir/build.gradle" ]] || [[ -f "$target_dir/build.gradle.kts" ]]; then
        echo "java"
        return 0
    fi

    # Check for Python project
    if [[ -f "$target_dir/pyproject.toml" ]] || [[ -f "$target_dir/Pipfile" ]] || [[ -f "$target_dir/requirements.txt" ]]; then
        echo "python"
        return 0
    fi

    # Check for Node.js project
    if [[ -f "$target_dir/package.json" ]]; then
        echo "node"
        return 0
    fi

    # Default to base
    echo "$DEFAULT_PROFILE"
}

#==============================================================================
# Session management
#==============================================================================

get_session_name() {
    local project_dir="${1:-.}"
    local abs_path
    abs_path=$(cd "$project_dir" && pwd)

    # Hash the path to create deterministic session name
    # Use base64 to make it readable but unique
    local hash
    hash=$(echo -n "$abs_path" | shasum -a 256 | cut -c1-8)

    local basename
    basename=$(basename "$abs_path")

    # Clean basename for tmux (alphanumeric, hyphen, underscore only)
    basename=$(echo "$basename" | sed 's/[^a-zA-Z0-9_-]/-/g')

    echo "hal-9000-${basename}-${hash}"
}

verify_prerequisites() {
    local missing_tools=()

    # Check for required tools
    for tool in docker bash; do
        if ! command -v "$tool" &> /dev/null; then
            missing_tools+=("$tool")
        fi
    done

    if [[ ${#missing_tools[@]} -gt 0 ]]; then
        error "Missing required tools: ${missing_tools[*]}"
    fi

    # Check Docker daemon
    if ! docker ps &> /dev/null; then
        error "Docker daemon not running or not accessible"
    fi

    # Check authentication - API key OR subscription login
    if [[ -z "${ANTHROPIC_API_KEY:-}" ]]; then
        # No API key - check for subscription login
        if check_subscription_auth; then
            success "Authentication: Subscription login detected"
        else
            echo ""
            warn "No authentication configured"
            echo ""
            info "Option 1: Subscription login (recommended)"
            echo "  hal-9000 /login"
            echo ""
            info "Option 2: API key"
            echo "  export ANTHROPIC_API_KEY=sk-ant-api03-..."
            echo "  Get key at: https://console.anthropic.com/settings/keys"
            echo ""
            error "Run 'hal-9000 /login' to authenticate with your Claude subscription"
        fi
    else
        success "Authentication: API key configured"
    fi

    success "Prerequisites verified"
}

#==============================================================================
# Session initialization
#==============================================================================

init_session() {
    local project_dir="${1:-.}"
    local profile="${2:-$(detect_profile "$project_dir")}"
    local session_name="${3:-$(get_session_name "$project_dir")}"
    local shell_mode="${4:-false}"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)

    # Ensure shared Docker volumes exist
    ensure_volumes

    # Create session metadata directory
    local session_dir="$HAL9000_HOME/claude/$session_name"
    mkdir -p "$session_dir"

    # Create session metadata
    cat > "$session_dir/.hal-9000-session.json" << METADATA
{
  "name": "$session_name",
  "profile": "$profile",
  "project_dir": "$abs_project_path",
  "created_at": "$(date -Iseconds)",
  "shell_mode": $shell_mode,
  "container_image": "${CONTAINER_IMAGE_BASE}:${profile}",
  "hal-9000_version": "$SCRIPT_VERSION"
}
METADATA

    echo "$session_name"
}

#==============================================================================
# Container launching
#==============================================================================

launch_container_session() {
    local session_name="$1"
    local profile="$2"
    local project_dir="$3"
    local shell_mode="$4"
    local api_key="$5"
    local abs_project_path

    abs_project_path=$(cd "$project_dir" && pwd)
    local container_image="${CONTAINER_IMAGE_BASE}:${profile}"

    # Verify image exists or fall back to base
    if ! docker inspect "$container_image" &> /dev/null; then
        warn "Image not found: $container_image, using base profile"
        container_image="${CONTAINER_IMAGE_BASE}:base"
    fi

    info "Launching Claude in container..."
    info "  Profile: $profile"
    info "  Session: $session_name"
    info "  Directory: $abs_project_path"

    # Build docker run command
    # Uses shared Docker volumes for CLAUDE_HOME and memory bank
    # All workers share these volumes, so plugins persist across sessions
    local docker_args=(
        docker run
        --rm
        -it
        --name "$session_name"
        -v "$abs_project_path:/workspace"
        -v "${CLAUDE_HOME_VOLUME}:/root/.claude"
        -v "${MEMORY_BANK_VOLUME}:/root/memory-bank"
        -v /var/run/docker.sock:/var/run/docker.sock
        -e "HAL9000_SESSION=$session_name"
        -e "HAL9000_PROJECT_DIR=$abs_project_path"
        -e "MEMORY_BANK_ROOT=/root/memory-bank"
    )

    # Pass through API key if available
    if [[ -n "$api_key" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$api_key")
        info "  Auth: API key"
    elif [[ -n "${ANTHROPIC_API_KEY:-}" ]]; then
        docker_args+=(-e "ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY")
        info "  Auth: API key (from environment)"
    else
        info "  Auth: Interactive login required"
    fi

    # Add image
    docker_args+=(-w "/workspace" "$container_image")

    # Run shell or Claude (using tmux inside container for session persistence)
    if [[ "$shell_mode" == "true" ]]; then
        info "Starting bash shell in tmux..."
        "${docker_args[@]}" tmux new-session -s shell
    else
        info "Starting Claude CLI in tmux..."
        "${docker_args[@]}" tmux new-session -s claude "claude"
    fi
}

#==============================================================================
# Main entry point
#==============================================================================

main() {
    local project_dir="."
    local profile=""
    local session_name=""
    local api_key=""
    local shell_mode=false
    local detach=false
    local verify_only=false
    local via_parent=false

    # Handle daemon subcommand first
    if [[ "${1:-}" == "daemon" ]]; then
        shift
        handle_daemon_command "$@"
        exit 0
    fi

    # Handle pool subcommand
    if [[ "${1:-}" == "pool" ]]; then
        shift
        handle_pool_command "$@"
        exit 0
    fi

    # Handle claude subcommand passthrough (plugin, mcp, doctor, etc.)
    if is_claude_subcommand "${1:-}"; then
        run_claude_command "$@"
        exit $?
    fi

    # Parse arguments for interactive session mode
    while [[ $# -gt 0 ]]; do
        case "$1" in
            --profile)
                profile="$2"
                shift 2
                ;;
            --shell|-s)
                shell_mode=true
                shift
                ;;
            --name)
                session_name="$2"
                shift 2
                ;;
            --api-key)
                api_key="$2"
                shift 2
                ;;
            --detach|-d)
                detach=true
                shift
                ;;
            --via-parent)
                via_parent=true
                shift
                ;;
            --verify)
                verify_only=true
                shift
                ;;
            --setup)
                setup_auth
                exit 0
                ;;
            --diagnose)
                show_diagnostics
                exit 0
                ;;
            --help|-h)
                show_help
                exit 0
                ;;
            --version|-v)
                show_version
                exit 0
                ;;
            -*)
                error "Unknown option: $1"
                ;;
            *)
                project_dir="$1"
                shift
                ;;
        esac
    done

    # Verify prerequisites (skip API key check for via-parent mode)
    if [[ "$via_parent" == "true" ]]; then
        # For via-parent, just check Docker is available
        if ! command -v docker &> /dev/null; then
            error "Docker not found"
        fi
        if ! docker ps &> /dev/null; then
            error "Docker daemon not running"
        fi
    else
        if ! verify_prerequisites; then
            exit 1
        fi
    fi

    if [[ "$verify_only" == "true" ]]; then
        exit 0
    fi

    # Change to project directory
    if [[ ! -d "$project_dir" ]]; then
        error "Directory not found: $project_dir"
    fi

    # Auto-detect profile if not specified
    if [[ -z "$profile" ]]; then
        profile=$(detect_profile "$project_dir")
        info "Auto-detected profile: $profile"
    fi

    # Initialize session
    if [[ -z "$session_name" ]]; then
        session_name=$(get_session_name "$project_dir")
    fi

    # Launch via parent or direct
    if [[ "$via_parent" == "true" ]]; then
        # DinD mode: spawn worker via parent container
        info "Using via-parent mode (DinD)"
        launch_via_parent "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" "$detach" || error "Failed to launch via parent"
    else
        # Direct mode: launch container directly
        info "Initializing session..."
        init_session "$project_dir" "$profile" "$session_name" "$shell_mode" || error "Failed to initialize session"

        success "Session ready: $session_name"

        # Launch container
        launch_container_session "$session_name" "$profile" "$project_dir" "$shell_mode" "$api_key" || error "Failed to launch container"
    fi
}

# Run main if script is executed directly
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    main "$@"
fi
